{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "parent_dir = os.path.abspath(\"../../\")\n",
    "sys.path.append(parent_dir)\n",
    "parent_dir = os.path.abspath(\"../../utils/\")\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\phi(\\boldsymbol{u})$ is a random feature map for $\\boldsymbol{u} \\in \\mathbb{R}^d$. For $\\boldsymbol{Q'}, \\boldsymbol{K'} \\in \\mathbb{R}^{L \\times r}$ with rows given as  $\\phi(\\boldsymbol{q_i}^T)^T$ and $\\phi(\\boldsymbol{k_i}^T)^T$ respectively, leads to the more efficient attention mechanism of the form:\n",
    " $$\n",
    "\\begin{aligned}\n",
    "  &\\mathrm{Att}_{\\!\\leftrightarrow}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})\n",
    "   = \\hat{\\mathbf{D}}^{-1}\\bigl(\\mathbf{Q}'((\\mathbf{K}')^{\\!\\top}\\mathbf{V})\\bigr),\n",
    "  &\n",
    "  &\\hat{\\mathbf{D}}\n",
    "   = \\operatorname{diag}\\!\\bigl(\\mathbf{Q}'((\\mathbf{K}')^{\\!\\top}\\mathbf{1}_L)\\bigr).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In the paper, they choose: \n",
    "$$\\phi(\\mathbf{x}) = \\frac{h(\\mathbf{x})}{\\sqrt{m}} \\left(f_1(\\omega_1^T \\mathbf{x}), \\dots, f_1(\\omega_m^T \\mathbf{x}), ..., f_l(\\omega_1^T \\mathbf{x}), ..., f_l(\\omega_m^T \\mathbf{x})\\right)$$\n",
    "\n",
    "Here are 3 possible combinations to try:\n",
    "1. $$\\hat{SM_{m}}^{trig} (\\mathbf{x}, \\mathbf{y}) := h(x) = exp\\left(\\frac{\\|\\mathbf{x}\\|^2}{2}\\right), l=2, f_1 = sin, f_2 = cos$$\n",
    "2. $$\\hat{SM_{m}}^{+} (\\mathbf{x}, \\mathbf{y}) := h(x) = exp\\left(-\\frac{\\|\\mathbf{x}\\|^2}{2}\\right), l=1, f_1 = exp$$\n",
    "3. $$\\hat{SM_{m}}^{hyp+} (\\mathbf{x}, \\mathbf{y}) := h(x) = \\frac{1}{\\sqrt{2}}exp\\left(-\\frac{\\|\\mathbf{x}\\|^2}{2}\\right), l=2, f_1 = exp(u), f_2 = exp(-u)$$\n",
    "\n",
    "\n",
    "How to choose $w_i$'s such that it is orthogonal:\n",
    "We want to choose different random samples $\\omega_1,...,\\omega_m$ such that they come from $D \\sim \\mathcal{N}(0, \\mathbf{I_d})$. We can use the Gram-Schmidt orthogonalisation to generate these. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_schmidt(vectors):\n",
    "    \"\"\"\n",
    "    Apply Gram-Schmidt orthogonalization to a set of vectors.\n",
    "    \n",
    "    Args:\n",
    "        vectors: numpy array of shape (m, n) where m is number of vectors\n",
    "                and n is the dimension of each vector\n",
    "    \n",
    "    Returns:\n",
    "        orthogonal_vectors: numpy array of orthogonalized vectors\n",
    "    \"\"\"\n",
    "    vectors = vectors.copy().astype(float)\n",
    "    m, n = vectors.shape\n",
    "    \n",
    "    for i in range(m):\n",
    "        vectors[i] = vectors[i] / np.linalg.norm(vectors[i])\n",
    "        for j in range(i + 1, m):\n",
    "            projection = np.dot(vectors[j], vectors[i]) * vectors[i]\n",
    "            vectors[j] = vectors[j] - projection\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "def generate_random_vectors_gaussian(m, n, mean=0, std=1):\n",
    "    \"\"\"Generate m random vectors from Gaussian distribution.\"\"\"\n",
    "    return np.random.normal(mean, std, (m, n))\n",
    "\n",
    "def generate_random_vectors_uniform(m, n, low=-1, high=1):\n",
    "    \"\"\"Generate m random vectors from uniform distribution.\"\"\"\n",
    "    return np.random.uniform(low, high, (m, n))\n",
    "\n",
    "def generate_random_vectors_multivariate_normal(m, n, mean=None, cov=None):\n",
    "    \"\"\"Generate m random vectors from multivariate normal distribution.\"\"\"\n",
    "    if mean is None:\n",
    "        mean = np.zeros(n)\n",
    "    if cov is None:\n",
    "        cov = np.eye(n)\n",
    "    \n",
    "    return np.random.multivariate_normal(mean, cov, m)\n",
    "\n",
    "def generate_orthogonal_random_vectors(m, n, distribution='gaussian', **kwargs):\n",
    "    \"\"\"\n",
    "    Generate m orthogonal random vectors of dimension n.\n",
    "    \n",
    "    Args:\n",
    "        m: number of vectors\n",
    "        n: dimension of each vector\n",
    "        distribution: 'gaussian', 'uniform', or 'multivariate_normal'\n",
    "        **kwargs: additional parameters for distribution\n",
    "    \n",
    "    Returns:\n",
    "        orthogonal_vectors: numpy array of orthogonalized random vectors\n",
    "    \"\"\"\n",
    "    if m > n:\n",
    "        raise ValueError(f\"Cannot generate {m} orthogonal vectors in {n}D space\")\n",
    "    \n",
    "    if distribution == 'gaussian':\n",
    "        vectors = generate_random_vectors_gaussian(m, n, **kwargs)\n",
    "    elif distribution == 'uniform':\n",
    "        vectors = generate_random_vectors_uniform(m, n, **kwargs)\n",
    "    elif distribution == 'multivariate_normal':\n",
    "        vectors = generate_random_vectors_multivariate_normal(m, n, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(\"Distribution must be 'gaussian', 'uniform', or 'multivariate_normal'\")\n",
    "    \n",
    "    return gram_schmidt(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_feature_map(omegas, x, kernel_type=\"trig\"):\n",
    "    if kernel_type == \"trig\":\n",
    "        num = np.exp(np.linalg.norm(x)**2 / 2) / np.sqrt(omegas.shape[0])\n",
    "        ws =  omegas @ x\n",
    "        f1, f2 = np.sin(ws), np.cos(ws)\n",
    "        return num * np.concatenate([f1, f2])\n",
    "    elif kernel_type == \"+\":\n",
    "        num = np.exp(-np.linalg.norm(x)**2 / 2) / np.sqrt(omegas.shape[0])\n",
    "        ws = omegas @ x\n",
    "        f1 = np.exp(ws)\n",
    "        return num * f1\n",
    "    elif kernel_type == \"hyp\":\n",
    "        num = np.exp(-np.linalg.norm(x)**2 / 2) / np.sqrt(2 * omegas.shape[0])\n",
    "        ws = omegas @ x\n",
    "        f1, f2 = np.exp(ws), np.exp(-ws)\n",
    "        return num * np.concatenate([f1, f2])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel type: {kernel_type}\")\n",
    "    \n",
    "# --- Random Feature Map Function (PyTorch version) ---\n",
    "def random_feature_map_torch(omegas_t, x_t, kernel_type=\"+\"):\n",
    "    \"\"\"\n",
    "    Computes random features for positive kernel, vectorized for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        omegas_t (torch.Tensor): Random projection matrix (m, d).\n",
    "        x_t (torch.Tensor): Input tensor (L, d).\n",
    "        kernel_type (str): Specifies the kernel type. Currently only \"+\" is implemented.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Random features (L, m).\n",
    "    \"\"\"\n",
    "    if kernel_type == \"+\":\n",
    "        norm_sq = torch.sum(x_t**2, dim=-1, keepdim=True)\n",
    "\n",
    "        # Apply scaling to omegas for numerical stability (1/sqrt(2))\n",
    "        ws = x_t @ (omegas_t.T / torch.sqrt(torch.tensor(2.0, device=x_t.device)))\n",
    "        f1 = torch.exp(ws)\n",
    "        num_term = torch.exp(-norm_sq / 2) / torch.sqrt(torch.tensor(omegas_t.shape[0], dtype=torch.float32, device=x_t.device))\n",
    "\n",
    "        return num_term * f1\n",
    "    elif kernel_type == \"trig\":\n",
    "        norm_sq = torch.sum(x_t**2, dim=-1, keepdim=True)\n",
    "        ws = x_t @ (omegas_t.T / torch.sqrt(torch.tensor(2.0, device=x_t.device)))\n",
    "        f1, f2 = torch.sin(ws), torch.cos(ws)\n",
    "        num_term = torch.exp(norm_sq / 2) /torch.sqrt(torch.tensor(omegas_t.shape[0], dtype=torch.float32, device=x_t.device))\n",
    "        return num_term * torch.cat([f1, f2], dim=-1)\n",
    "    elif kernel_type == \"hyp\":\n",
    "        norm_sq = torch.sum(x_t**2, dim=-1, keepdim=True)\n",
    "        ws = x_t @ (omegas_t.T / torch.sqrt(torch.tensor(2.0, device=x_t.device)))\n",
    "        f1, f2 = torch.exp(ws), torch.exp(-ws)\n",
    "        num_term = torch.exp(-norm_sq / 2) / torch.sqrt(torch.tensor(2.0 * omegas_t.shape[0], dtype=torch.float32, device=x_t.device))\n",
    "        return num_term * torch.cat([f1, f2], dim=-1)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Kernel type '{kernel_type}' is not implemented for PyTorch conversion.\")\n",
    "\n",
    "    \n",
    "def iid_gaussian_torch(rows, cols, device):\n",
    "    return torch.randn(rows, cols, device=device)\n",
    "\n",
    "def orthogonal_gaussian_torch(m, d, device):\n",
    "    \"\"\"\n",
    "    Generates a matrix composed of stacked orthogonal blocks.\n",
    "    \n",
    "    Args:\n",
    "        m (int): The desired number of rows (vectors).\n",
    "        d (int): The dimension of each vector.\n",
    "        device (torch.device): The device (e.g., 'cpu' or 'cuda') to place the tensors on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (m, d) where blocks of d rows are orthogonal.\n",
    "    \"\"\"\n",
    "    def orthogonal_square():\n",
    "        q, _ = torch.linalg.qr(iid_gaussian_torch(d, d, device))\n",
    "        return q.T\n",
    "\n",
    "    num_squares = int(m / d)\n",
    "    blocks = [orthogonal_square() for _ in range(num_squares)]\n",
    "\n",
    "    remainder = m - d * num_squares\n",
    "    if remainder > 0:\n",
    "        blocks.append(orthogonal_square()[:remainder])\n",
    "    matrix = torch.cat(blocks, dim=0)\n",
    "    divisor = torch.sqrt(torch.tensor(float(num_squares) + float(remainder) / d, device=device))\n",
    "    matrix /= divisor\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def my_favour(query, key, value, m_omegas, omegas, kernel_type=\"trig\"):\n",
    "    _, L, d = query.shape\n",
    "    query_norm = query / (d ** 0.25)\n",
    "    key_norm = key / (d ** 0.25)\n",
    "\n",
    "    if kernel_type == \"trig\" or kernel_type == \"hyp\":\n",
    "        feature_dimension = 2 * m_omegas\n",
    "    else: # For kernel_type == \"+\"\n",
    "        feature_dimension = m_omegas\n",
    "\n",
    "    new_query = np.zeros((L, feature_dimension))\n",
    "    new_key = np.zeros((L, feature_dimension))\n",
    "\n",
    "    for i in range(L):\n",
    "        new_query[i] = random_feature_map(omegas, query_norm[i], kernel_type=kernel_type)\n",
    "        new_key[i] = random_feature_map(omegas, key_norm[i], kernel_type=kernel_type)\n",
    "\n",
    "    C = np.column_stack([value, np.ones((L, 1))])\n",
    "    buf_1 = new_key.T @ C\n",
    "    buf_2 = new_query @ buf_1\n",
    "    buf_3 = buf_2[:, :-1]\n",
    "    buf_4 = buf_2[:, -1]\n",
    "    favor_output = buf_3 / buf_4[:, np.newaxis]\n",
    "    return favor_output\n",
    "\n",
    "\n",
    "def pytorch_favor(q, k, v, omegas, device, kernel_type=\"trig\"):\n",
    "    L, d = q.shape\n",
    "    query_scaled_input = q / (d ** 0.25)\n",
    "    key_scaled_input = k / (d ** 0.25)\n",
    "    new_query = random_feature_map_torch(omegas, query_scaled_input, kernel_type=kernel_type)\n",
    "    new_key = random_feature_map_torch(omegas, key_scaled_input, kernel_type=kernel_type)\n",
    "\n",
    "    C = torch.cat([v, torch.ones(L, 1, device=device)], dim=1)\n",
    "    buf_1 = new_key.T @ C\n",
    "    buf_2 = new_query @ buf_1\n",
    "    buf_3 = buf_2[:, :-1]\n",
    "    buf_4 = buf_2[:, -1]\n",
    "    ans = buf_3 / buf_4.unsqueeze(-1)\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lx/bw9zm4ts5xqgrty123rxm60w0000gn/T/ipykernel_4545/3235992820.py:123: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3729.)\n",
      "  buf_1 = new_key.T @ C\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (64) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(B, L, d, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     14\u001b[0m omegas \u001b[38;5;241m=\u001b[39m orthogonal_gaussian_torch(m, d, device)\n\u001b[0;32m---> 17\u001b[0m ans_pt \u001b[38;5;241m=\u001b[39m \u001b[43mpytorch_favor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momegas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhyp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m ans_np \u001b[38;5;241m=\u001b[39m my_favour(query\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), key\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), value\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), m, omegas\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), kernel_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m attn_ref_logits \u001b[38;5;241m=\u001b[39m query \u001b[38;5;241m@\u001b[39m key\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m/\u001b[39m (d \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[57], line 123\u001b[0m, in \u001b[0;36mpytorch_favor\u001b[0;34m(q, k, v, omegas, device, kernel_type)\u001b[0m\n\u001b[1;32m    120\u001b[0m new_key \u001b[38;5;241m=\u001b[39m random_feature_map_torch(omegas, key_scaled_input, kernel_type\u001b[38;5;241m=\u001b[39mkernel_type)\n\u001b[1;32m    122\u001b[0m C \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v, torch\u001b[38;5;241m.\u001b[39mones(B, L, \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 123\u001b[0m buf_1 \u001b[38;5;241m=\u001b[39m \u001b[43mnew_key\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\n\u001b[1;32m    124\u001b[0m buf_2 \u001b[38;5;241m=\u001b[39m new_query \u001b[38;5;241m@\u001b[39m buf_1\n\u001b[1;32m    125\u001b[0m buf_3 \u001b[38;5;241m=\u001b[39m buf_2[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1024) must match the size of tensor b (64) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "L = 140   # Sequence length / Batch size\n",
    "d = 512   # Embedding dimension\n",
    "m = d  # Number of random features\n",
    "B = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# if device.type == 'cuda':\n",
    "#     torch.cuda.manual_seed_all(42)\n",
    "\n",
    "query = torch.randn(B, L, d, device=device)\n",
    "key = torch.randn(B, L, d, device=device)\n",
    "value = torch.randn(B, L, d, device=device)\n",
    "omegas = orthogonal_gaussian_torch(m, d, device)\n",
    "\n",
    "\n",
    "ans_pt = pytorch_favor(query, key, value, omegas, device, kernel_type=\"hyp\")\n",
    "ans_np = my_favour(query.cpu().numpy(), key.cpu().numpy(), value.cpu().numpy(), m, omegas.cpu().numpy(), kernel_type=\"hyp\")\n",
    "\n",
    "attn_ref_logits = query @ key.T / (d ** 0.5)\n",
    "attn_ref = F.softmax(attn_ref_logits, dim=-1)\n",
    "attn_ref = attn_ref @ value\n",
    "is_close = torch.allclose(ans_pt, attn_ref, atol=1e-3, rtol=1e-3)\n",
    "\n",
    "print(f\"\\nAre PyTorch `ans` (Linear Attention) and reference `attn_ref` (Softmax Attention) close? {is_close}\")\n",
    "print(f\"Max absolute difference: {torch.max(torch.abs(ans_pt - attn_ref)):.6f}\")\n",
    "print(f\"Mean absolute difference: {torch.mean(torch.abs(ans_pt - attn_ref)):.6f}\")\n",
    "\n",
    "print(f\"Max absolute difference: {torch.max(torch.abs(torch.tensor(ans_np) - attn_ref)):.6f}\")\n",
    "print(f\"Mean absolute difference: {torch.mean(torch.abs(torch.tensor(ans_np)  - attn_ref)):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prefix sum implementation\n",
    "G_temp = np.einsum('lm,ln->lmn', new_key, C)\n",
    "G = np.cumsum(G_temp, axis=0)\n",
    "buf_2 = np.einsum('lm,lmd->ld', new_query, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are they close? False\n",
      "Max absolute difference: 0.815663\n",
      "Mean absolute difference: 0.103693\n",
      "Are they close? False\n",
      "Max absolute difference: 0.815663\n",
      "Mean absolute difference: 0.103693\n",
      "Are they close? True\n",
      "Max absolute difference: 0.000000\n",
      "Mean absolute difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "q = np.random.randn(L, d)\n",
    "k = np.random.randn(L, d)\n",
    "v = np.random.randn(L, d)\n",
    "m =  1 * d\n",
    "random_feats = orthogonal_gaussian(m, d)\n",
    "true_val = att(q, k, v)\n",
    "guess = positive_att_hat(q, k, v, random_feats)\n",
    "my_guess = my_favour(q, k, v, d, random_feats)\n",
    "is_close = np.allclose(\n",
    "    true_val, guess, \n",
    ")\n",
    "\n",
    "print(f\"Are they close? {is_close}\")\n",
    "print(f\"Max absolute difference: {np.max(np.abs(true_val - guess)):.6f}\")\n",
    "print(f\"Mean absolute difference: {np.mean(np.abs(true_val - guess)):.6f}\")\n",
    "\n",
    "is_close = np.allclose(\n",
    "    true_val, my_guess, \n",
    ")\n",
    "\n",
    "print(f\"Are they close? {is_close}\")\n",
    "print(f\"Max absolute difference: {np.max(np.abs(true_val - my_guess)):.6f}\")\n",
    "print(f\"Mean absolute difference: {np.mean(np.abs(true_val - my_guess)):.6f}\")\n",
    "\n",
    "\n",
    "print(f\"Are they close? {np.allclose(guess, my_guess)}\")\n",
    "print(f\"Max absolute difference: {np.max(np.abs(guess - my_guess)):.6f}\")\n",
    "print(f\"Mean absolute difference: {np.mean(np.abs(guess - my_guess)):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist((torch.tensor(favor_output, dtype=torch.float32) - regular_attention_output).reshape(-1), bins=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Vanila Transformer attention implementation\n",
    "def att(q, k, v, normalize=True):\n",
    "    l, d = q.shape\n",
    "    normalizer = 1 / (d ** 0.5) if normalize else 1\n",
    "    a = np.exp(q @ k.T * normalizer)\n",
    "    d_inv = np.diag(1 / (a @ np.ones(l)))\n",
    "    return d_inv @ a @ v\n",
    "\n",
    "\n",
    "# Perfomer attention implementation using some random feature map phi\n",
    "def att_hat(q, k, v, phi, normalize=True):\n",
    "    l, d = q.shape\n",
    "    normalizer = 1 / (d ** 0.25)\n",
    "    q_prime = phi(q * normalizer)\n",
    "    k_prime = phi(k * normalizer)\n",
    "    d_inv = np.diag(1 / (q_prime @ (k_prime.T @ np.ones(l))))\n",
    "    return d_inv @ (q_prime @ (k_prime.T @ v))\n",
    "\n",
    "\n",
    "# random feature map\n",
    "def phi(h, fs, random_feats):\n",
    "    return lambda x: (\n",
    "        h(x)\n",
    "        / np.sqrt(m)\n",
    "        * np.concatenate(\n",
    "            [f(np.einsum(\"...d,md->...m\", x, random_feats)) for f in fs],\n",
    "            axis=-1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Performer \"positive\" attention\n",
    "def positive_att_hat(q, k, v, random_feats, normalize=True):\n",
    "    def h(x):\n",
    "        return np.exp(-np.square(x).sum(axis=-1, keepdims=True) / 2)\n",
    "\n",
    "    kernel = phi(h, [np.exp], random_feats)\n",
    "    return att_hat(q, k, v, kernel, normalize)\n",
    "\n",
    "\n",
    "# generate IID Gaussian random features\n",
    "def iid_gaussian(m, d):\n",
    "    return np.random.normal(size=(m, d))\n",
    "\n",
    "\n",
    "# generate orthogonal Gaussian random features\n",
    "def orthogonal_gaussian(m, d):\n",
    "    def orthogonal_square():\n",
    "        # create orthogonal square matrix using Gram-Schmidt\n",
    "        q, _ = np.linalg.qr(iid_gaussian(d, d))\n",
    "        return q.T\n",
    "\n",
    "    num_squares = int(m / d)\n",
    "    blocks = [orthogonal_square() for _ in range(num_squares)]\n",
    "\n",
    "    remainder = m - d * num_squares\n",
    "    if remainder:\n",
    "        blocks.append(orthogonal_square()[:remainder])\n",
    "\n",
    "    matrix = np.vstack(blocks)\n",
    "    matrix /= np.sqrt(num_squares + remainder / d)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# mean squared error\n",
    "def mse(a, b):\n",
    "    return np.square(a - b).mean()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
