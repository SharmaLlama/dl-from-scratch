{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import sentencepiece as spm\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "def extract_long_translations(\n",
    "        min_length: int = 200,\n",
    "        max_length: int = 2000,\n",
    "        sp_model_path: str = \"../attention_is_all_you_need/BPE/en-hi.model\",\n",
    "        max_samples: int | None = None\n",
    "    ) -> Dict[str, str]:\n",
    "    # ── 0.  SentencePiece initialisation ───────────────────────────────\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    if not Path(sp_model_path).exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"SentencePiece model not found at {sp_model_path}\"\n",
    "        )\n",
    "    sp.load(sp_model_path)                              # turn0search6\n",
    "\n",
    "    def sp_len(txt: str) -> int:\n",
    "        return len(sp.encode_as_ids(txt))\n",
    "\n",
    "    # Helper to add (en,hi) pairs if they meet length and quota\n",
    "    results: dict[str, str] = {}\n",
    "\n",
    "    def maybe_add(en: str, hi: str) -> None:\n",
    "        if  min_length <= sp_len(en) <= max_length and min_length <= sp_len(hi) <= max_length:\n",
    "            if max_samples is None or len(results) < max_samples:\n",
    "                results[en] = hi\n",
    "\n",
    "    # Helper that walks through *any* HF dataset object\n",
    "    def walk_dataset(ds, nested_translation: bool) -> None:\n",
    "        if isinstance(ds, DatasetDict):\n",
    "            splits = ds.values()\n",
    "        else:                           # bare Dataset\n",
    "            splits = [ds]\n",
    "        for split in splits:\n",
    "            for ex in split:\n",
    "                if nested_translation:                  # translation column\n",
    "                    maybe_add(ex[\"translation\"][\"en\"],\n",
    "                              ex[\"translation\"][\"hi\"])\n",
    "                else:                                   # src / tgt style\n",
    "                    maybe_add(ex[\"src\"], ex[\"tgt\"])\n",
    "\n",
    "    # ── 1.  OPUS-100 (≈ 55 M pairs, en-hi subset) ──────────────────────\n",
    "    walk_dataset(\n",
    "        load_dataset(\"opus100\", \"en-hi\"),                # turn0search0\n",
    "        nested_translation=True,\n",
    "    )\n",
    "\n",
    "    # ── 2.  IITB EN-HI corpus ──────────────────────────────────────────\n",
    "    walk_dataset(\n",
    "        load_dataset(\"cfilt/iitb-english-hindi\"),        # turn0search1\n",
    "        nested_translation=True,\n",
    "    )\n",
    "\n",
    "    # ── 3.  Samanantar (AI4Bharat) ─────────────────────────────────────\n",
    "    walk_dataset(\n",
    "        load_dataset(\"ai4bharat/samanantar\", \"hi\"),      # turn0search2\n",
    "        nested_translation=False,                       # uses src / tgt\n",
    "    )\n",
    "\n",
    "    # ── 4.  PMIndiaSum: align the *two* directions ─────────────────────\n",
    "    #        (Hindi article + EN summary)  ↔  (English article + HI summary)\n",
    "    ds_hi = load_dataset(\"PMIndiaData/PMIndiaSum\", data_dir=\"hindi-english\")\n",
    "    ds_en = load_dataset(\"PMIndiaData/PMIndiaSum\", data_dir=\"english-hindi\")\n",
    "\n",
    "    # 4-A.  Build a single lookup table from *every* English-side split\n",
    "    en_by_url = {}\n",
    "    for split_name, split in ds_en.items():             # train / validation / test\n",
    "        for ex in split:\n",
    "            en_by_url[ex[\"source_url\"]] = ex            # keep last seen, fine for 1-1 mapping\n",
    "\n",
    "    # 4-B.  Iterate over every Hindi-side split and align\n",
    "    for split_name, split in ds_hi.items():\n",
    "        for ex_hi in split:\n",
    "            eng_rec = en_by_url.get(ex_hi[\"target_url\"])\n",
    "            if eng_rec:\n",
    "                maybe_add(eng_rec[\"text\"], ex_hi[\"text\"])\n",
    "                \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = extract_long_translations(50, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_encoded = sp.Encode(list(sentences.keys()))\n",
    "hindi_encoded = sp.Encode(list(sentences.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 816,270\n",
      "Combined length - Min: 50, Max: 1997\n",
      "Average combined length: 76.6 tokens\n",
      "0-100 tokens: 715,935 pairs (87.7%)\n",
      "100-200 tokens: 89,813 pairs (11.0%)\n",
      "200-300 tokens: 6,615 pairs (0.8%)\n",
      "300-500 tokens: 2,126 pairs (0.3%)\n",
      "500-750 tokens: 760 pairs (0.1%)\n",
      "750-1000 tokens: 412 pairs (0.1%)\n",
      "1000-1500 tokens: 435 pairs (0.1%)\n",
      "2000+ tokens: 0 pairs (0.0%)\n"
     ]
    }
   ],
   "source": [
    "def analyze_encoded_lengths(english_encoded, hindi_encoded):\n",
    "    \n",
    "    # Calculate lengths\n",
    "    en_lengths = [len(seq) for seq in english_encoded]\n",
    "    hi_lengths = [len(seq) for seq in hindi_encoded]\n",
    "    combined_lengths = [en for en, hi in zip(en_lengths, hi_lengths)]\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Total pairs: {len(combined_lengths):,}\")\n",
    "    print(f\"Combined length - Min: {min(combined_lengths)}, Max: {max(combined_lengths)}\")\n",
    "    print(f\"Average combined length: {sum(combined_lengths)/len(combined_lengths):.1f} tokens\")\n",
    "    \n",
    "    # Count by length buckets\n",
    "    buckets = [(0, 100), (100, 200), (200, 300), (300, 500), (500, 750), (750, 1000), (1000, 1500),  (2000, float('inf'))]\n",
    "    for min_len, max_len in buckets:\n",
    "        count = sum(1 for l in combined_lengths if min_len <= l < max_len)\n",
    "        label = f\"{min_len}-{max_len}\" if max_len != float('inf') else f\"{min_len}+\"\n",
    "        print(f\"{label} tokens: {count:,} pairs ({count/len(combined_lengths)*100:.1f}%)\")\n",
    "    \n",
    "# Use with your data:\n",
    "lengths = analyze_encoded_lengths(english_encoded, hindi_encoded)\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DOWNLOADING HINDI LITERATURE DATASETS ===\n",
      "\n",
      "1. Downloading OPUS Books...\n",
      "Downloading Books/v1/moses/en-hi.txt.zip...\n",
      "Failed to download Books/v1/moses/en-hi.txt.zip\n",
      "Downloading QED/v2.0a/moses/en-hi.txt.zip...\n",
      "Failed to download QED/v2.0a/moses/en-hi.txt.zip\n",
      "Downloading TED2020/v1/moses/en-hi.txt.zip...\n",
      "Failed to download TED2020/v1/moses/en-hi.txt.zip\n",
      "\n",
      "2. Project Gutenberg Literature:\n",
      "\n",
      "3. Additional Academic Sources:\n",
      "\n",
      "=== MANUAL DOWNLOAD INSTRUCTIONS ===\n",
      "For the richest literature datasets:\n",
      "1. Visit https://opus.nlpl.eu/Books.php\n",
      "2. Download EN-HI parallel books\n",
      "3. Visit Digital Library of India for classical texts\n",
      "4. Contact academic institutions for research datasets\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "\n",
    "def download_opus_books():\n",
    "    \"\"\"Download OPUS Books corpus (literature translations)\"\"\"\n",
    "    \n",
    "    urls = [\n",
    "        \"https://opus.nlpl.eu/download.php?f=Books/v1/moses/en-hi.txt.zip\",\n",
    "        \"https://opus.nlpl.eu/download.php?f=QED/v2.0a/moses/en-hi.txt.zip\",  # Educational content\n",
    "        \"https://opus.nlpl.eu/download.php?f=TED2020/v1/moses/en-hi.txt.zip\"  # TED talks (longer speeches)\n",
    "    ]\n",
    "    \n",
    "    for url in urls:\n",
    "        filename = url.split('=')[-1]\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            # Extract\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(f\"extracted_{filename.replace('.zip', '')}\")\n",
    "            \n",
    "            print(f\"Extracted to extracted_{filename.replace('.zip', '')}/\")\n",
    "        else:\n",
    "            print(f\"Failed to download {filename}\")\n",
    "\n",
    "def download_gutenberg_literature():\n",
    "    \"\"\"Download specific Gutenberg texts with Hindi translations\"\"\"\n",
    "    \n",
    "    # These are known to have Hindi translations\n",
    "    gutenberg_books = [\n",
    "        {\n",
    "            'title': 'Alice in Wonderland',\n",
    "            'en_url': 'https://www.gutenberg.org/files/11/11-0.txt',\n",
    "            'hi_info': 'Search Project Gutenberg for Hindi translation'\n",
    "        },\n",
    "        {\n",
    "            'title': 'Aesop Fables', \n",
    "            'en_url': 'https://www.gutenberg.org/files/21/21-0.txt',\n",
    "            'hi_info': 'Available in Hindi on Gutenberg'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for book in gutenberg_books:\n",
    "        print(f\"Book: {book['title']}\")\n",
    "        print(f\"English: {book['en_url']}\")\n",
    "        print(f\"Hindi: {book['hi_info']}\\n\")\n",
    "\n",
    "def create_literature_corpus_from_files(directory_path):\n",
    "    \"\"\"Process downloaded literature files and create long-sequence corpus\"\"\"\n",
    "    \n",
    "    literature_pairs = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.en'):  # English file\n",
    "                en_file = os.path.join(root, file)\n",
    "                hi_file = os.path.join(root, file.replace('.en', '.hi'))\n",
    "                \n",
    "                if os.path.exists(hi_file):\n",
    "                    with open(en_file, 'r', encoding='utf-8') as ef:\n",
    "                        with open(hi_file, 'r', encoding='utf-8') as hf:\n",
    "                            en_lines = ef.readlines()\n",
    "                            hi_lines = hf.readlines()\n",
    "                            \n",
    "                            # Combine multiple sentences into paragraphs for longer sequences\n",
    "                            for i in range(0, len(en_lines), 3):  # Group every 3 sentences\n",
    "                                if i + 2 < len(hi_lines):\n",
    "                                    en_para = ' '.join([en_lines[j].strip() for j in range(i, min(i+3, len(en_lines)))])\n",
    "                                    hi_para = ' '.join([hi_lines[j].strip() for j in range(i, min(i+3, len(hi_lines)))])\n",
    "                                    \n",
    "                                    if len(en_para.split()) + len(hi_para.split()) > 200:\n",
    "                                        literature_pairs.append({\n",
    "                                            'english': en_para,\n",
    "                                            'hindi': hi_para,\n",
    "                                            'source': file,\n",
    "                                            'length': len(en_para.split()) + len(hi_para.split())\n",
    "                                        })\n",
    "    \n",
    "    return literature_pairs\n",
    "\n",
    "def download_additional_sources():\n",
    "    \"\"\"Download from additional academic/cultural sources\"\"\"\n",
    "    \n",
    "    print(\"Additional Hindi Literature Sources:\")\n",
    "    print(\"1. Digital Library of India: https://dli.iiit.ac.in/\")\n",
    "    print(\"2. Sanskrit Documents: https://sanskritdocuments.org/\")\n",
    "    print(\"3. Hindi Samay Magazine Archive: http://www.hindisamay.com/\")\n",
    "    print(\"4. Rekhta (Urdu-Hindi): https://rekhta.org/\")\n",
    "    print(\"5. Kavita Kosh: http://kavitakosh.org/\")\n",
    "    \n",
    "    # These require manual download or API access\n",
    "    academic_sources = [\n",
    "        {\n",
    "            'name': 'Indian Institute of Science Archive',\n",
    "            'url': 'https://archive.org/details/iisc',\n",
    "            'description': 'Historical texts with translations'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Sahitya Akademi Digital Archive', \n",
    "            'url': 'https://sahitya-akademi.gov.in/',\n",
    "            'description': 'Official literature translations'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for source in academic_sources:\n",
    "        print(f\"\\n{source['name']}: {source['url']}\")\n",
    "        print(f\"Description: {source['description']}\")\n",
    "\n",
    "def test_sentencepiece_length(text_pairs, model_path=None):\n",
    "    \"\"\"Test how SentencePiece affects your sequence lengths\"\"\"\n",
    "    \n",
    "    try:\n",
    "        import sentencepiece as spm\n",
    "        \n",
    "        if model_path and os.path.exists(model_path):\n",
    "            sp = spm.SentencePieceProcessor(model_file=model_path)\n",
    "        else:\n",
    "            print(\"No SentencePiece model provided, using word-based analysis\")\n",
    "            return\n",
    "            \n",
    "        print(\"SentencePiece Length Analysis:\")\n",
    "        for i, pair in enumerate(text_pairs[:10]):  # Test first 10\n",
    "            en_tokens = sp.encode(pair['english'])\n",
    "            hi_tokens = sp.encode(pair['hindi']) \n",
    "            total_sp_length = len(en_tokens) + len(hi_tokens)\n",
    "            word_length = len(pair['english'].split()) + len(pair['hindi'].split())\n",
    "            \n",
    "            print(f\"Pair {i+1}: Words={word_length}, SentencePiece={total_sp_length}\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"SentencePiece not installed. Install with: pip install sentencepiece\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== DOWNLOADING HINDI LITERATURE DATASETS ===\\n\")\n",
    "    \n",
    "    # Download OPUS literature collections\n",
    "    print(\"1. Downloading OPUS Books...\")\n",
    "    download_opus_books()\n",
    "    \n",
    "    # Show Gutenberg options\n",
    "    print(\"\\n2. Project Gutenberg Literature:\")\n",
    "    # download_gutenberg_literature()\n",
    "    \n",
    "    # Show additional sources\n",
    "    print(\"\\n3. Additional Academic Sources:\")\n",
    "    # download_additional_sources()\n",
    "    \n",
    "    print(\"\\n=== MANUAL DOWNLOAD INSTRUCTIONS ===\")\n",
    "    print(\"For the richest literature datasets:\")\n",
    "    print(\"1. Visit https://opus.nlpl.eu/Books.php\")\n",
    "    print(\"2. Download EN-HI parallel books\")\n",
    "    print(\"3. Visit Digital Library of India for classical texts\")\n",
    "    print(\"4. Contact academic institutions for research datasets\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
