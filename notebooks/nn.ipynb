{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    def __init__(self, value, op=None, children=()):\n",
    "        self.value = value\n",
    "        self.grad = 0\n",
    "        self.is_leaf = not children\n",
    "        self.children = children\n",
    "        self.op = op\n",
    "        self._backward = lambda : None\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Scalar) else Scalar(other)\n",
    "        out = Scalar(self.value + other.value, children=(self, other), op='+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "        \n",
    "    def __sub__(self, other):\n",
    "        other = other if isinstance(other, Scalar) else Scalar(other)\n",
    "        out = Scalar(self.value - other.value, op='-',  children=(self, other))\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad -= 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Scalar) else Scalar(other)\n",
    "        out = Scalar(self.value * other.value, op='*',  children=(self, other))\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return other * self\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other ** -1)\n",
    "            \n",
    "    def __pow__(self, other):\n",
    "        out = Scalar(self.value ** other, children=(self, ), op='power')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * self.value ** (other - 1) * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v.children:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "        # self.grad = 1\n",
    "        # self._backward()\n",
    "        # child_list  = [c for c in self.children]\n",
    "        # while child_list:\n",
    "        #     curr = child_list.pop(0)\n",
    "        #     curr._backward()\n",
    "        #     if not curr.is_leaf:\n",
    "        #         for d in curr.children:\n",
    "        #             child_list.append(d)\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.value\n",
    "        out = Scalar(np.exp(x), children=(self, ), op='exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * out.value\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.value\n",
    "        fun = lambda x: (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "        out = Scalar(fun(x), children=(self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 - fun(x)**2)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        x = self.value\n",
    "        fun = lambda x: 1/(1 + np.exp(-x))\n",
    "        out = Scalar(fun(x), children=(self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * fun(x) * (1-fun(x))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        x = self.value\n",
    "        out = Scalar(np.max([0, x]), children=(self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 if x > 0 else 0\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def leaky_relu(self, alpha=0.1):\n",
    "        x = self.value\n",
    "        out = Scalar(np.max([x, -alpha*x]), children=(self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 if x > 0 else alpha\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Scalar(data: {self.value})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Scalar(2.1)\n",
    "b = Scalar(0.8)\n",
    "c = a * b\n",
    "d = c * 0.25\n",
    "e = d.tanh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Scalar(data: 2.1),\n",
       " Scalar(data: 0.8),\n",
       " Scalar(data: 1.6800000000000002),\n",
       " Scalar(data: 0.42000000000000004),\n",
       " Scalar(data: 0.39693043200507755))"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c,d,e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Scalar(x) for x in np.random.normal(size=nin)]\n",
    "        self.b = Scalar(0.0)\n",
    "\n",
    "    \n",
    "    def __call__(self, input, activation='relu', args=None):\n",
    "        assert len(input) == len(self.w), \"input size is not equal to weights initialisation\"\n",
    "        out = sum((wi * xi for wi, xi in zip(self.w, input)), self.b)\n",
    "        if activation == 'relu':\n",
    "            post_act = out.relu()\n",
    "        elif activation == 'leaky_relu':\n",
    "            alpha = 0.1 if args is None else args \n",
    "            post_act = out.leaky_relu(alpha=alpha)\n",
    "        elif activation == 'sigmoid':\n",
    "            post_act = out.sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            post_act = out.tanh()\n",
    "        else: \n",
    "            raise ValueError(\"Incorrect Activation Function provided\")\n",
    "        \n",
    "        return post_act\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "    def parameters_count(self):\n",
    "        return len(self.w) + 1\n",
    "    \n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, lsize, nin):\n",
    "        self.neurons = [Neuron(nin) for _ in range(lsize)]\n",
    "\n",
    "    def __call__(self, *args, activation='relu'):\n",
    "        layer_out = [neuron(args[0], activation=activation) for neuron in self.neurons]\n",
    "        return layer_out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "    \n",
    "    def parameters_count(self):\n",
    "        return sum(x.parameters_count() for x in self.neurons)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, lsizes, nin):\n",
    "        sz = [nin] + lsizes\n",
    "        self.layers = [Layer(sz[i+1], sz[i]) for i in range(len(lsizes))]\n",
    "\n",
    "    def __call__(self, *args, activation='relu'):\n",
    "        x = args[0]\n",
    "        for layer in self.layers:\n",
    "            x  = layer(x, activation=activation)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "    def parameters_count(self):\n",
    "        return sum(x.parameters_count() for x in self.layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0], \n",
    "    [3.0, -1, 0.5], \n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1]\n",
    "]\n",
    "\n",
    "ys = [1, -1, -1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP([4, 4, 1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_t = []\n",
    "for _ in range(20):\n",
    "    ypred = [model(x, activation='tanh')[0] for x in xs]\n",
    "    loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "    for p in model.parameters():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.value += -0.05 * p.grad\n",
    "    \n",
    "    loss_t.append(loss.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Scalar(data: 0.8483610396224256),\n",
       " Scalar(data: -0.8188368351386829),\n",
       " Scalar(data: -0.8274681586473587),\n",
       " Scalar(data: 0.8212747397934365)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13ff43a30>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(loss_t[:], '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Equations Derivations\n",
    "Let us consider a simple 2-layer perceptron with input $x_1 \\in \\mathbb{R}^{N_1 \\times 1}$ and some non-linear activation functions. We have the following equations, along with their shapes:\n",
    "$$a_1 = W_1 x_1 + b_1, \\hspace{10pt} W_1 \\in \\mathbb{R}^{M_1 \\times N_1}  \\hspace{5pt} b_1, a_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$ h_1 = f_1(a_1), \\hspace{10pt} h_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    "$$a_2 = W_2 h_1 + b_2, \\hspace{10pt} W_2 \\in \\mathbb{R}^{M_2 \\times M_1}  \\hspace{5pt} b_2, a_2 \\in \\mathbb{R}^{M_2 \\times 1} $$\n",
    " $$ y_2 = f_2(a_2), \\hspace{10pt} y_2 \\in \\mathbb{R}^{M_2 \\times 1} $$\n",
    " $$L = \\frac{1}{2}(y_2 - y_g)^2 \\hspace{10pt} L \\in \\mathbb{R}^{M_2 \\times 1}$$\n",
    "\n",
    "\n",
    " Now, we consider the back-propagation step, starting with the first derivatives:\n",
    " $$\\frac{\\partial L} {\\partial y_2} = y_2 - y_g$$\n",
    " $$\\frac{\\partial L} {\\partial a_2} = \\frac{\\partial L} {\\partial y_2} \\cdot \\frac{\\partial y_2} {\\partial y_2} =  (y_2 - y_g) \\odot f_2'(a_2)$$\n",
    "  $$\\boxed{\\frac{\\partial L} {\\partial W_2} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial W_2} =   (y_2 - y_g) \\odot f_2'(a_2) \\times h_1^T}$$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_2} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial b_2} =   (y_2 - y_g) \\odot f_2'(a_2) \\times 1}$$\n",
    "$$\\frac{\\partial L} {\\partial h_1} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial h_1} =  W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right)  $$\n",
    "$$\\frac{\\partial L} {\\partial a_1} = \\frac{\\partial L} {\\partial h_1} \\cdot \\frac{\\partial h_1} {\\partial a_1} =    W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1) $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial W_1} = \\frac{\\partial L} {\\partial a_1} \\cdot \\frac{\\partial a_1} {\\partial W_1} =    W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1) \\times x_1^T} $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_1} = \\frac{\\partial L} {\\partial a_1} \\cdot \\frac{\\partial a_1} {\\partial b_1} =  W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1)}$$\n",
    "\n",
    "\n",
    "Now, let us do this generically for an n-layer neural network:\n",
    "$$a_1 = W_1 x_1 + b_1, \\hspace{10pt} W_1 \\in \\mathbb{R}^{M_1 \\times N_1}  \\hspace{5pt} b_1, a_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$ h_1 = f_1(a_1), \\hspace{10pt} h_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$a_j = W_j h_{j-1} + b_j, \\hspace{10pt} W_j \\in \\mathbb{R}^{M_{j} \\times M_{j-1}}  \\hspace{5pt} b_j, a_j \\in \\mathbb{R}^{M_j \\times 1} $$\n",
    " $$ h_j = f_j(a_j), \\hspace{10pt} y_j \\in \\mathbb{R}^{M_j \\times 1} $$\n",
    "$$a_N = W_N h_{N-1} + b_N, \\hspace{10pt} W_n \\in \\mathbb{R}^{M_{N} \\times M_{N-1}}  \\hspace{5pt} b_N, a_N \\in \\mathbb{R}^{M_N \\times 1} $$\n",
    " $$ y_N = f_N(a_N), \\hspace{10pt} y_N \\in \\mathbb{R}^{M_N \\times 1} $$\n",
    " $$L = (y_N - y_g)^2 \\hspace{10pt} L \\in \\mathbb{R}^{M_N \\times 1}$$\n",
    "\n",
    " $$\\boxed{\\frac{\\partial L} {\\partial W_j} = \\frac{\\partial L} {\\partial a_j} \\cdot \\frac{\\partial a_j} {\\partial W_j} =   \\left(W_{j-1}^T \\times \\left(...W_N \\times (y_N - y_g) \\odot f_N'(a_N)\\right)... \\odot f_{N-1}'(a_{N-1}) \\right) \\odot f_j'(a_j) \\times h_{j-1}^T} $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_j} = \\frac{\\partial L} {\\partial a_j} \\cdot \\frac{\\partial a_j} {\\partial b_j} =   \\left(W_{j-1}^T \\times \\left(...W_N \\times (y_N - y_g) \\odot f_N'(a_N)\\right)... \\odot f_{N-1}'(a_{N-1}) \\right) \\odot f_j'(a_j)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Equations Derivations (With Batches)\n",
    "Now, let us consider a simple 2-layer perceptron with input $x_1 \\in \\mathbb{R}^{B \\times N_1 \\times 1}$ and some non-linear activation functions. We have the following equations, along with their shapes:\n",
    "$$a_1 = W_1 x_1 + b_1, \\hspace{10pt} W_1 \\in \\mathbb{R}^{M_1 \\times N_1}  \\hspace{5pt} b_1, a_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$ h_1 = f_1(a_1), \\hspace{10pt} h_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    "$$a_2 = W_2 h_1 + b_2, \\hspace{10pt} W_2 \\in \\mathbb{R}^{M_2 \\times M_1}  \\hspace{5pt} b_2, a_2 \\in \\mathbb{R}^{M_2 \\times 1} $$\n",
    " $$ y_2 = f_2(a_2), \\hspace{10pt} y_2 \\in \\mathbb{R}^{M_2 \\times 1} $$\n",
    " $$L = \\frac{1}{2}(y_2 - y_g)^2 \\hspace{10pt} L \\in \\mathbb{R}^{M_2 \\times 1}$$\n",
    "\n",
    "\n",
    " Now, we consider the back-propagation step, starting with the first derivatives:\n",
    " $$\\frac{\\partial L} {\\partial y_2} = y_2 - y_g$$\n",
    " $$\\frac{\\partial L} {\\partial a_2} = \\frac{\\partial L} {\\partial y_2} \\cdot \\frac{\\partial y_2} {\\partial y_2} =  (y_2 - y_g) \\odot f_2'(a_2)$$\n",
    "  $$\\boxed{\\frac{\\partial L} {\\partial W_2} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial W_2} =   (y_2 - y_g) \\odot f_2'(a_2) \\times h_1^T}$$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_2} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial b_2} =   (y_2 - y_g) \\odot f_2'(a_2) \\times 1}$$\n",
    "$$\\frac{\\partial L} {\\partial h_1} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial h_1} =  W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right)  $$\n",
    "$$\\frac{\\partial L} {\\partial a_1} = \\frac{\\partial L} {\\partial h_1} \\cdot \\frac{\\partial h_1} {\\partial a_1} =    W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1) $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial W_1} = \\frac{\\partial L} {\\partial a_1} \\cdot \\frac{\\partial a_1} {\\partial W_1} =    W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1) \\times x_1^T} $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_1} = \\frac{\\partial L} {\\partial a_1} \\cdot \\frac{\\partial a_1} {\\partial b_1} =  W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1)}$$\n",
    "\n",
    "\n",
    "Now, let us do this generically for an n-layer neural network:\n",
    "$$a_1 = W_1 x_1 + b_1, \\hspace{10pt} W_1 \\in \\mathbb{R}^{M_1 \\times N_1}  \\hspace{5pt} b_1, a_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$ h_1 = f_1(a_1), \\hspace{10pt} h_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$a_j = W_j h_{j-1} + b_j, \\hspace{10pt} W_j \\in \\mathbb{R}^{M_{j} \\times M_{j-1}}  \\hspace{5pt} b_j, a_j \\in \\mathbb{R}^{M_j \\times 1} $$\n",
    " $$ h_j = f_j(a_j), \\hspace{10pt} y_j \\in \\mathbb{R}^{M_j \\times 1} $$\n",
    "$$a_N = W_N h_{N-1} + b_N, \\hspace{10pt} W_n \\in \\mathbb{R}^{M_{N} \\times M_{N-1}}  \\hspace{5pt} b_N, a_N \\in \\mathbb{R}^{M_N \\times 1} $$\n",
    " $$ y_N = f_N(a_N), \\hspace{10pt} y_N \\in \\mathbb{R}^{M_N \\times 1} $$\n",
    " $$L = (y_N - y_g)^2 \\hspace{10pt} L \\in \\mathbb{R}^{M_N \\times 1}$$\n",
    "\n",
    " $$\\boxed{\\frac{\\partial L} {\\partial W_j} = \\frac{\\partial L} {\\partial a_j} \\cdot \\frac{\\partial a_j} {\\partial W_j} =   \\left(W_{j-1}^T \\times \\left(...W_N \\times (y_N - y_g) \\odot f_N'(a_N)\\right)... \\odot f_{N-1}'(a_{N-1}) \\right) \\odot f_j'(a_j) \\times h_{j-1}^T} $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_j} = \\frac{\\partial L} {\\partial a_j} \\cdot \\frac{\\partial a_j} {\\partial b_j} =   \\left(W_{j-1}^T \\times \\left(...W_N \\times (y_N - y_g) \\odot f_N'(a_N)\\right)... \\odot f_{N-1}'(a_{N-1}) \\right) \\odot f_j'(a_j)}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, input):\n",
    "        self.tensor = np.array(input)\n",
    "        self.size = self.tensor.shape\n",
    "\n",
    "\n",
    "class LayerT:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.biases = np.zeros(shape=(nout, 1))\n",
    "        self.weights = np.random.uniform(-np.sqrt(6/(nin + nout)), np.sqrt(6/(nin + nout)), size=(nout, nin))\n",
    "        self.weights_grad = 0.0 * self.weights\n",
    "        self.biases_grad = 0.0 * self.biases\n",
    "        self.x = None\n",
    "        \n",
    "    def _broadcast_rule_checker(self, x, y):\n",
    "        x_shape = x.shape\n",
    "        y_shape = y.shape\n",
    "\n",
    "        for i in range(1, min(len(x_shape), len(y_shape))+1):\n",
    "            if x_shape[-i] != y_shape[-i]: \n",
    "                raise ValueError(f\"Dimensions do not match: dim {-i}\")\n",
    "        \n",
    "        while len(x_shape) < len(y_shape):\n",
    "            x = np.expand_dims(x, 0)\n",
    "        while len(y_shape) < len(x_shape):\n",
    "            y = np.expand_dims(y, 0)\n",
    "        return x,y\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return self.weights @ x + self.biases\n",
    "        \n",
    "    def parameters_count(self):\n",
    "        return self.weights.size + self.biases.size\n",
    "    \n",
    "    def _backward(self):\n",
    "        return \n",
    "        \n",
    "class NonLinearity(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def fun(self, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod \n",
    "    def _backward(self):\n",
    "        pass\n",
    "\n",
    "class Tanh(NonLinearity):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = None\n",
    "        \n",
    "    def fun(self, x):\n",
    "        self.x = x\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def __call__(self, x):\n",
    "         return self.fun(x)\n",
    "    \n",
    "    def _backward(self):\n",
    "        return (1- self.fun(self.x)**2)\n",
    "    \n",
    "class Relu(NonLinearity):\n",
    "    def __init__(self, alpha=0.0):\n",
    "        self.alpha = alpha\n",
    "        self.x = None\n",
    "            \n",
    "    def fun(self, x):\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.fun(x)\n",
    "        \n",
    "    def _backward(self):\n",
    "         return np.where(self.x > 0, 1.0, self.alpha)\n",
    "        \n",
    "\n",
    "class Sigmoid(NonLinearity):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = None\n",
    "\n",
    "    def fun(self, x):\n",
    "        self.x = x\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.fun(x)\n",
    "    \n",
    "    def _backward(self):\n",
    "        return self.fun(self.x) * (1-self.fun(self.x))\n",
    "    \n",
    "class NormLoss:\n",
    "    def __init__(self, pow=2):\n",
    "        self.pow = pow\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return 1/self.pow * np.sum(np.abs(x-y)**self.pow)\n",
    "    \n",
    "    def _backward(self):\n",
    "        if self.x is None:\n",
    "            raise ValueError(\"No Forward Pass Called\")\n",
    "        if self.x.ndim > 0: \n",
    "            return (self.pow - 1) * (self.x-self.y)\n",
    "        else: \n",
    "            return (self.pow - 1) * np.sum(self.x - self.y, -1)\n",
    "    \n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers=[]):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, l):\n",
    "        l = [l] if not isinstance(l, list) else l\n",
    "        self.layers.extend(l)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = np.array(x)\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(-1, 1)\n",
    "        else: \n",
    "            x = x.T\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, loss : NormLoss):\n",
    "        acc_grad = loss._backward()\n",
    "        temp_grad = None\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, NonLinearity):\n",
    "                acc_grad = acc_grad * layer._backward()\n",
    "            elif isinstance(layer, LayerT):\n",
    "                temp_grad = acc_grad\n",
    "                # print(\"bias matrix grad shape:\", temp_grad.shape)\n",
    "                # print(\"layer x:\", layer.x.T.shape)\n",
    "                acc_grad = temp_grad @ layer.x.T\n",
    "                # print(\"post h multipy shape:\", acc_grad.shape)\n",
    "                layer.weights +=  -0.1 * acc_grad\n",
    "                layer.biases += -0.1 * np.sum(temp_grad, axis=-1, keepdims=True)\n",
    "                # print(layer.weights_grad.shape == layer.weights.shape)\n",
    "                # print(layer.biases_grad.shape == layer.biases.shape)\n",
    "                acc_grad = layer.weights.T @ temp_grad\n",
    "                # print(\"post W multipy shape:\", acc_grad.shape)\n",
    "        return acc_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6891168713450764"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = LayerT(3, 10)\n",
    "s1 = Sigmoid()\n",
    "s2 = Sigmoid()\n",
    "s3 = Sigmoid()\n",
    "s4 = Sigmoid()\n",
    "b = LayerT(10, 20)\n",
    "c = LayerT(20, 5)\n",
    "d = LayerT(5, 1)\n",
    "e = Tanh()\n",
    "l = NormLoss()\n",
    "nn = NeuralNetwork([a, s1, b,s2, c, s3, d, e])\n",
    "l(nn(xs), ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(500):\n",
    "    nn.backward(l)\n",
    "    l(nn(xs), ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0030450148726526858"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(nn(xs), ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparsion between Backprop Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets:\n",
    "    def __init__(self, x_features, x_samples):\n",
    "        self.xs = np.random.random(size=(x_samples, x_features)) * 10 - 5\n",
    "        self.ys = (np.random.rand(x_samples) > 0.5) * 2 - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNeuralNets:\n",
    "    def __init__(self, xs, ys, depth, max_width=100):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.hidden_layer_sizes = np.random.randint(low=20, high=max_width, size=depth)\n",
    "        self.n1 = self.create_basic_nn(depth, max_width, self.hidden_layer_sizes)\n",
    "        self.n2 = self.create_tensor_nn(depth, max_width, self.hidden_layer_sizes)\n",
    "\n",
    "    def create_basic_nn(self):\n",
    "        return MLP(self.hidden_layer_sizes, nin=self.xs.shape[1])\n",
    "    \n",
    "    def create_tensor_nn(self): \n",
    "        layers = [LayerT(self.xs.shape[1], self.hidden_layer_sizes[0]), Tanh()]\n",
    "        for i in self.hidden_layer_sizes[:-1]:\n",
    "            layers.append(LayerT(self.hidden_layer_sizes[i], self.hidden_layer_sizes[i+1]))\n",
    "            layers.append(Tanh())\n",
    "        layers.append(LayerT(self.hidden_layer_sizes[-1], 1), Tanh())\n",
    "        return NeuralNetwork(layers=layers)\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
