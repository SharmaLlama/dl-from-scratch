{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    def __init__(self, value, op=None, children=()):\n",
    "        self.value = value\n",
    "        self.grad = 0\n",
    "        self.is_leaf = not children\n",
    "        self.children = children\n",
    "        self.op = op\n",
    "        self._backward = lambda : None\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Scalar) else Scalar(other)\n",
    "        out = Scalar(self.value + other.value, children=(self, other), op='+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "        \n",
    "    def __sub__(self, other):\n",
    "        other = other if isinstance(other, Scalar) else Scalar(other)\n",
    "        out = Scalar(self.value - other.value, op='-',  children=(self, other))\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad -= 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Scalar) else Scalar(other)\n",
    "        out = Scalar(self.value * other.value, op='*',  children=(self, other))\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return other * self\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other ** -1)\n",
    "            \n",
    "    def __pow__(self, other):\n",
    "        out = Scalar(self.value ** other, children=(self, ), op='power')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * self.value ** (other - 1) * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v.children:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "        # self.grad = 1\n",
    "        # self._backward()\n",
    "        # child_list  = [c for c in self.children]\n",
    "        # while child_list:\n",
    "        #     curr = child_list.pop(0)\n",
    "        #     curr._backward()\n",
    "        #     if not curr.is_leaf:\n",
    "        #         for d in curr.children:\n",
    "        #             child_list.append(d)\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.value\n",
    "        out = Scalar(np.exp(x), children=(self, ), op='exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * out.value\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.value\n",
    "        fun = lambda x: (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "        out = Scalar(fun(x), children=(self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 - fun(x)**2)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        x = self.value\n",
    "        fun = lambda x: 1/(1 + np.exp(-x))\n",
    "        out = Scalar(fun(x), children=(self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * fun(x) * (1-fun(x))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        x = self.value\n",
    "        out = Scalar(np.max([0, x]), children=(self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 if x > 0 else 0\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def leaky_relu(self, alpha=0.1):\n",
    "        x = self.value\n",
    "        out = Scalar(np.max([x, -alpha*x]), children=(self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 if x > 0 else alpha\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Scalar(data: {self.value})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Scalar(2.1)\n",
    "b = Scalar(0.8)\n",
    "c = a * b\n",
    "d = c * 0.25\n",
    "e = d.tanh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Scalar(data: 2.1),\n",
       " Scalar(data: 0.8),\n",
       " Scalar(data: 1.6800000000000002),\n",
       " Scalar(data: 0.42000000000000004),\n",
       " Scalar(data: 0.39693043200507755))"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c,d,e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Scalar(x) for x in np.random.normal(size=nin)]\n",
    "        self.b = Scalar(0.0)\n",
    "\n",
    "    \n",
    "    def __call__(self, input, activation='relu', args=None):\n",
    "        assert len(input) == len(self.w), \"input size is not equal to weights initialisation\"\n",
    "        out = sum((wi * xi for wi, xi in zip(self.w, input)), self.b)\n",
    "        if activation == 'relu':\n",
    "            post_act = out.relu()\n",
    "        elif activation == 'leaky_relu':\n",
    "            alpha = 0.1 if args is None else args \n",
    "            post_act = out.leaky_relu(alpha=alpha)\n",
    "        elif activation == 'sigmoid':\n",
    "            post_act = out.sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            post_act = out.tanh()\n",
    "        else: \n",
    "            raise ValueError(\"Incorrect Activation Function provided\")\n",
    "        \n",
    "        return post_act\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, lsize, nin):\n",
    "        self.neurons = [Neuron(nin) for _ in range(lsize)]\n",
    "\n",
    "    def __call__(self, *args, activation='relu'):\n",
    "        layer_out = [neuron(args[0], activation=activation) for neuron in self.neurons]\n",
    "        return layer_out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, lsizes, nin):\n",
    "        sz = [nin] + lsizes\n",
    "        self.layers = [Layer(sz[i+1], sz[i]) for i in range(len(lsizes))]\n",
    "\n",
    "    def __call__(self, *args, activation='relu'):\n",
    "        x = args[0]\n",
    "        for layer in self.layers:\n",
    "            x  = layer(x, activation=activation)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0], \n",
    "    [3.0, -1, 0.5], \n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1]\n",
    "]\n",
    "\n",
    "ys = [1, -1, -1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP([4, 4, 1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_t = []\n",
    "for _ in range(20):\n",
    "    ypred = [model(x, activation='tanh')[0] for x in xs]\n",
    "    loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "    for p in model.parameters():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.value += -0.05 * p.grad\n",
    "    \n",
    "    loss_t.append(loss.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Scalar(data: 0.9179116258156363),\n",
       " Scalar(data: -0.9671157822285359),\n",
       " Scalar(data: -0.9200399208773009),\n",
       " Scalar(data: 0.929924199466702)]"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17a789f90>]"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAun0lEQVR4nO3deXxU9b3/8feZmWSykAxbQogECAGCCFIEqohbtdJitXptXXotRdveVi9upf1doX3c6u199Mbe7q0Vlx+i3rbir1ex3p9Wiz9ZbBULiJWiZV+i7BEzWcgkmfn+/sjMJBOyTXJmTmbm9Xw8ziMzZ77nzOd4yCNvzznf79cyxhgBAADYwOV0AQAAIH0QLAAAgG0IFgAAwDYECwAAYBuCBQAAsA3BAgAA2IZgAQAAbEOwAAAAtvEk+wtDoZAOHTqkgoICWZaV7K8HAAD9YIxRXV2dSktL5XJ1f10i6cHi0KFDKisrS/bXAgAAG1RXV2vMmDHdfp70YFFQUCCprbDCwsJkfz0AAOgHv9+vsrKy6N/x7iQ9WERufxQWFhIsAABIMb09xsDDmwAAwDYECwAAYBuCBQAAsA3BAgAA2IZgAQAAbEOwAAAAtiFYAAAA2xAsAACAbQgWAADANgQLAABgG4IFAACwDcECAADYJi2CRTBk9Ku1u3XXqq061Rx0uhwAADJWWgQLt8vSyj/v0+/fPqTdx+qdLgcAgIyVFsFCkiaPapsffsfROocrAQAgc6VdsNhJsAAAwDFpFyx2HCFYAADglLQJFpUlQyRxxQIAACelTbCYFL5icbi2SbWnWhyuBgCAzJQ2waIwJ0ulvhxJ0u5jXLUAAMAJaRMsJGlySeQ5C7qcAgDghLiDxQcffKAvfvGLGjFihPLy8vSxj31MW7ZsSURtcaukZwgAAI7yxNP45MmTmjdvnj7xiU/oD3/4g4qLi7Vnzx4NHTo0QeXFZxI9QwAAcFRcweIHP/iBysrKtHLlyui68ePH211Tv3HFAgAAZ8V1K+T555/X7Nmzdd1116m4uFgzZ87Uo48+2uM2gUBAfr8/ZkmUicVDZFlSTUOzTtQHEvY9AACga3EFi71792r58uWaNGmSXn75Zd16662688479eSTT3a7TVVVlXw+X3QpKysbcNHdyc12a9zwPEnSTm6HAACQdJYxxvS1cXZ2tmbPnq3XX389uu7OO+/Upk2b9MYbb3S5TSAQUCDQfvXA7/errKxMtbW1KiwsHEDpXfvak5v1x3eP6r6rpurmeeW27x8AgEzk9/vl8/l6/fsd1xWL0aNHa+rUqTHrzjzzTB08eLDbbbxerwoLC2OWRKqMdDk9SpdTAACSLa5gMW/ePO3YsSNm3c6dOzVu3DhbixqISTzACQCAY+IKFt/4xje0ceNG/cd//Id2796t3/72t3rkkUe0ePHiRNUXt2jPkCN1iuMuDwAAsEFcwWLOnDlavXq1nnrqKU2bNk3//u//rp/97Ge66aabElVf3MpH5svjslQXaNXh2ianywEAIKPENY6FJF155ZW68sorE1GLLbI9Lk0oytfOo/XacbROpUNznS4JAICMkVZzhURM7nA7BAAAJE9aBov2ETjpGQIAQDKlZbCIzHJKzxAAAJIrPYNF+IrFrmN1CoboGQIAQLKkZbAYOzxPXo9LTS0hVX/Y6HQ5AABkjLQMFm6XpUmjhkiSdnA7BACApEnLYCHRMwQAACekbbCI9gw5Rs8QAACSJW2DRbRnCFcsAABImvQNFuErFnuO16u5NeRwNQAAZIa0DRalvhwN8XrUGjLaX9PgdDkAAGSEtA0WlmVpcqRnCLdDAABIirQNFpJUyQicAAAkVVoHi8hzFlyxAAAgOdI6WFRGh/amyykAAMmQ1sEi0uV0f02DmlqCDlcDAED6S+tgMXKIV8Pzs2WMtJurFgAAJFxaBwtJ9AwBACCJ0j5YRIf2pmcIAAAJl/bBIvKcBbOcAgCQeGkfLCqZ5RQAgKRJ+2AxKRwsDtU2qa6pxeFqAABIb2kfLHy5WSopzJEk7TxKzxAAABIp7YOF1GEKdZ6zAAAgoTIiWFTS5RQAgKTIiGAxmS6nAAAkRUYEC2Y5BQAgOTIiWEwsHiLLkk7UN6umPuB0OQAApK2MCBZ52R6NHZ4niZ4hAAAkUkYEC0maVMztEAAAEi1jgkVlSbhnCMECAICEyZhgMZmhvQEASLiMCRaVHSYjM8Y4XA0AAOkpY4LFhJFD5HFZqmtq1RF/k9PlAACQljImWGR7XCofmS+JniEAACRKxgQLiecsAABItIwMFvQMAQAgMTIqWES6nDKWBQAAiZFRwaLjZGShED1DAACwW0YFi3Ej8pXtcampJaTqk41OlwMAQNrJqGDhdlmaVBwegZMHOAEAsF1GBQtJqgzfDtl1jC6nAADYLeOCxaRIzxCuWAAAYLu4gsV9990ny7JilpKSkkTVlhD0DAEAIHE88W5w1lln6ZVXXom+d7vdthaUaJGeIXuO16slGFKWO+Mu2gAAkDBxBwuPx5NyVyk6OmNorvKz3WpoDmr/iYborREAADBwcf/v+q5du1RaWqry8nLdeOON2rt3b4/tA4GA/H5/zOIky7I0uYQROAEASIS4gsW5556rJ598Ui+//LIeffRRHTlyROeff75qamq63aaqqko+ny+6lJWVDbjogaqMDpRFzxAAAOwUV7BYsGCBPve5z2n69On65Cc/qRdeeEGS9MQTT3S7zbJly1RbWxtdqqurB1axDSYxGRkAAAkR9zMWHeXn52v69OnatWtXt228Xq+8Xu9AvsZ2lR2G9gYAAPYZUJeIQCCg9957T6NHj7arnqSYHO5yur+mQU0tQYerAQAgfcQVLL71rW9p/fr12rdvn9588019/vOfl9/v16JFixJVX0IUDfFqWF6WQkbazQicAADYJq5g8f777+sLX/iCKisrde211yo7O1sbN27UuHHjElVfQliWFTPTKQAAsEdcz1isWrUqUXUkXWVJgd7c9yFdTgEAsFHGDjsZuWKxiy6nAADYJuODBZORAQBgnwwOFm09Qz746JTqmlocrgYAgPSQscFiaF62RhW2ja+xi54hAADYImODhdR+O4QROAEAsEdGB4vICJz0DAEAwB4ZHSwis5wylgUAAPbI6GDBLKcAANgro4PFxOK2niHH6wL6sKHZ4WoAAEh9GR0s8r0elQ3PlcTtEAAA7JDRwUJiCnUAAOyU8cGCETgBALBPxgeLSnqGAABgm4wPFpM79AwxxjhcDQAAqS3jg8WEony5XZZqT7XoWF3A6XIAAEhpGR8svB63xo/Ik8RzFgAADFTGBwuJ5ywAALALwUL0DAEAwC4ECzGWBQAAdiFYqONkZPUKhegZAgBAfxEsJI0bnqdsj0unWoL64KNTTpcDAEDKIlhI8rhdqihqm5CM5ywAAOg/gkVY5ahwsOA5CwAA+o1gETaZLqcAAAwYwSKski6nAAAMGMEiLDKWxd7jDWoNhhyuBgCA1ESwCDtjaK7ys91qDoa0v6bR6XIAAEhJBIswl8vSRAbKAgBgQAgWHUR7hvCcBQAA/UKw6GAyVywAABgQgkUHkVlOGcsCAID+IVh0EOlyuv9Eg5pagg5XAwBA6iFYdFBU4NXQvCyFjLTneL3T5QAAkHIIFh1YlhV9zmLXUYIFAADxIlh0Mpk5QwAA6DeCRSeR5yx20uUUAIC4ESw6idwK4YoFAADxI1h0EgkW7588pfpAq8PVAACQWggWnQzLz1ZxgVeStIurFgAAxIVg0YXIQFmMwAkAQHwIFl2YVBwJFnQ5BQAgHgSLLlSWtHU55YoFAADxIVh0IdozhC6nAADEZUDBoqqqSpZl6e6777apnMFhUjhYHKsL6GRDs8PVAACQOvodLDZt2qRHHnlEZ599tp31DApDvB6NGZYridshAADEo1/Bor6+XjfddJMeffRRDRs2zO6aBoXoCJwECwAA+qxfwWLx4sX6zGc+o09+8pN21zNoTC6hZwgAAPHyxLvBqlWr9NZbb2nTpk19ah8IBBQIBKLv/X5/vF/pCCYjAwAgfnFdsaiurtZdd92lX//618rJyenTNlVVVfL5fNGlrKysX4Um2+QOt0KMMQ5XAwBAarBMHH81n3vuOf3DP/yD3G53dF0wGJRlWXK5XAoEAjGfSV1fsSgrK1Ntba0KCwttOITEaGoJaup3X1LISH/59mUqLuxbkAIAIB35/X75fL5e/37HdSvksssu07Zt22LW3XLLLZoyZYruueee00KFJHm9Xnm93ni+ZlDIyXJr/Mh87T3eoB1H6wgWAAD0QVzBoqCgQNOmTYtZl5+frxEjRpy2Ph1UjipoCxZH6nThpCKnywEAYNBj5M0eTKbLKQAAcYm7V0hn69ats6GMwak9WNDlFACAvuCKRQ8ik5HtOlqnUIieIQAA9IZg0YNxI/KV7XapoTmoDz465XQ5AAAMegSLHmS5XZpQlC+J5ywAAOgLgkUvKsNDezMCJwAAvSNY9CL6AOcRggUAAL0hWPQiMsvpDnqGAADQK4JFLyJXLPYcr1drMORwNQAADG4Ei16MGZar3Cy3mltDOvBho9PlAAAwqBEseuFyWdEp1HnOAgCAnhEs+mDyKHqGAADQFwSLPoh0OWUsCwAAekaw6IPoFQtuhQAA0COCRR9MCj9jsb+mUc2t9AwBAKA7BIs+KCnM0RCvR8GQ0YGaBqfLAQBg0CJY9IFlWaoIzxmy5zgDZQEA0B2CRR9VFLXdDtl9jGABAEB3CBZ9VFFMsAAAoDcEiz6aGA4We47zjAUAAN0hWPRRe7CoVyhkHK4GAIDBiWDRR2OH58njstTYHNRhf5PT5QAAMCgRLPooy+3S+JFtPUN4zgIAgK4RLOIwMdwzZA/BAgCALhEs4lBRHL5iwVgWAAB0iWARh4l0OQUAoEcEizhMLGqbjIxbIQAAdI1gEYcJ4WG9axqadbKh2eFqAAAYfAgWccj3elTqy5HEnCEAAHSFYBEnhvYGAKB7BIs4dRyBEwAAxCJYxImeIQAAdI9gEafo9OlcsQAA4DQEizhFrli8f/KUmlqCDlcDAMDgQrCI04j8bA3Ny5Ix0l6mUAcAIAbBIk6WZXE7BACAbhAs+iEyGRkPcAIAEItg0Q/RLqcECwAAYhAs+oGxLAAA6BrBoh8iz1jsPdGgYMg4XA0AAIMHwaIfzhiWK6/HpebWkKo/bHS6HAAABg2CRT+4XZYmFHE7BACAzggW/cTQ3gAAnI5g0U8VRfmSCBYAAHREsOin6BULboUAABBFsOinjmNZGEPPEAAApDiDxfLly3X22WersLBQhYWFmjt3rv7whz8kqrZBbfyIfLksyd/UquP1AafLAQBgUIgrWIwZM0b333+/Nm/erM2bN+vSSy/V1Vdfre3btyeqvkErJ8utsuF5knjOAgCAiLiCxVVXXaUrrrhCkydP1uTJk/X9739fQ4YM0caNGxNV36AWmTOEob0BAGjT72csgsGgVq1apYaGBs2dO7fbdoFAQH6/P2ZJF+1DezN9OgAAUj+CxbZt2zRkyBB5vV7deuutWr16taZOndpt+6qqKvl8vuhSVlY2oIIHkwpmOQUAIEbcwaKyslJvv/22Nm7cqNtuu02LFi3Su+++2237ZcuWqba2NrpUV1cPqODBpIJBsgAAiOGJd4Ps7GxNnDhRkjR79mxt2rRJP//5z/Xwww932d7r9crr9Q6sykEqcivkiL9J9YFWDfHG/Z8TAIC0MuBxLIwxCgQys7ulLzdLRQVtoYkHOAEAiPOKxbe//W0tWLBAZWVlqqur06pVq7Ru3Tq99NJLiapv0KsoytfxuoB2H6vXjLKhTpcDAICj4goWR48e1cKFC3X48GH5fD6dffbZeumll3T55Zcnqr5Bb2LxEG3c+yFDewMAoDiDxYoVKxJVR8piLAsAANoxV8gAVTAZGQAAUQSLAYr0DDlQ06jm1pDD1QAA4CyCxQCVFOZoiNejYMjoQA0jcAIAMhvBYoAsy1JFUb4kaQ+3QwAAGY5gYQOG9gYAoA3BwgYM7Q0AQBuChQ2Y5RQAgDYECxu0B4t6hULG4WoAAHAOwcIGY4fnyeOy1Ngc1GF/k9PlAADgGIKFDbLcLo0f2dYzhOcsAACZjGBhE4b2BgCAYGGbiuLwFQvGsgAAZDCChU0m0uUUAACChV0mFhVI4lYIACCzESxsMiE8rHdNQ7NONjQ7XA0AAM4gWNgk3+tRqS9HEnOGAAAyF8HCRgztDQDIdAQLG3UcgRMAgExEsLARPUMAAJmOYGGj6PTpXLEAAGQogoWNIlcs3j95Sk0tQYerAQAg+QgWNhqRn62heVkyRtrLFOoAgAxEsLCRZVncDgEAZDSChc0ik5HxACcAIBMRLGwW7XJKsAAAZCCChc0YywIAkMkIFjaLPGOx90SDgiHjcDUAACQXwcJmZwzLldfjUnNrSNUfNjpdDgAASUWwsJnbZWlCEbdDAACZiWCRAAztDQDIVASLBKgoypdEsAAAZB6CRQJEr1hwKwQAkGEIFgnQcSwLY+gZAgDIHASLBBg/Il8uS/I3tep4fcDpcgAASBqCRQLkZLlVNjxPEs9ZAAAyC8EiQSJzhjC0NwAgkxAsEqR9aG+mTwcAZA6CRYJUMMspACADESwSpIJBsgAAGYhgkSCRWyFH/E2qD7Q6XA0AAMlBsEgQX26Wigq8kniAEwCQOQgWCcTQ3gCATEOwSCCG9gYAZJq4gkVVVZXmzJmjgoICFRcX65prrtGOHTsSVVvKYywLAECmiStYrF+/XosXL9bGjRu1Zs0atba2av78+WpoYKyGrlRwxQIAkGE88TR+6aWXYt6vXLlSxcXF2rJliy666CJbC0sHkVshB2oa1dwaUraHO08AgPQWV7DorLa2VpI0fPjwbtsEAgEFAu0Tcfn9/oF8ZUopKczREK9H9YFWHahp0KRRBU6XBABAQvX7f6GNMVqyZIkuuOACTZs2rdt2VVVV8vl80aWsrKy/X5lyLMuK9gzZw+0QAEAG6HewuP322/XOO+/oqaee6rHdsmXLVFtbG12qq6v7+5UpiaG9AQCZpF+3Qu644w49//zz2rBhg8aMGdNjW6/XK6/X26/i0gFDewMAMklcwcIYozvuuEOrV6/WunXrVF5enqi60gaznAIAMklcwWLx4sX67W9/q9///vcqKCjQkSNHJEk+n0+5ubkJKTDVtQeLeoVCRi6X5XBFAAAkTlzPWCxfvly1tbW65JJLNHr06Ojy9NNPJ6q+lDd2eJ48LkuNzUEd9jc5XQ4AAAkV960QxCfL7dL4kfnafaxeu4/V64yhXNkBAKQvRmxKAob2BgBkCoJFElQUh2c5ZSwLAECaI1gkwUS6nAIAMgTBIgkmFrUN5c2tEABAuiNYJMGE8LDeNQ3NOtnQ7HA1AAAkDsEiCfK9HpX6ciQxZwgAIL0RLJKEob0BAJmAYJEkHUfgBAAgXREskoSeIQCATECwSJLo9OlcsQAApDGCRZJErli8f/KUmlqCDlcDAEBiECySZER+tobmZckYaS9TqAMA0hTBIkksy+J2CAAg7REskigyGRkPcAIA0hXBIomiXU4JFgCANEWwSCLGsgAApDuCRRJFnrHYe6JBwZBxuBoAAOxHsEiiM4blyutxqbk1pOoPG50uBwAA2xEsksjtsjSBBzgBAGmMYJFkPGcBAEhnBIskqyjKl8QVCwBAeiJYJFl0MjKuWAAA0hDBIsk6jmVhDD1DAADphWCRZONH5MtlSf6mVh2vDzhdDgAAtiJYJFlOlltlw/Mk8ZwFACD9ECwcEJkzhKG9AQDphmDhgPYup0yfDgBILwQLB1QwSBYAIE0RLBxQUUywAACkJ4KFAyK3Qo74m1TX1OJwNQAA2Idg4QBfbpaKCrySpL08ZwEASCMEC4cwtDcAIB0RLBzC0N4AgHREsHAIY1kAANIRwcIhFVyxAACkIYKFQyK3Qg7UNKq5NeRwNQAA2INg4ZCSwhwN8XoUDBkdqKFnCAAgPRAsHGJZVrRnyB5uhwAA0gTBwkEM7Q0ASDcECwcxtDcAIN0QLBzEWBYAgHRDsHBQdPr0Yw0KhYzD1QAAMHAECweNHZ4nj8vSqZagDvubnC4HAIABiztYbNiwQVdddZVKS0tlWZaee+65BJSVGbLcLo0fyZwhAID0EXewaGho0IwZM/TAAw8kop6Mw9DeAIB04ol3gwULFmjBggWJqCUjVRTnS9t5gBMAkB7iDhbxCgQCCgQC0fd+vz/RX5lSJtLlFACQRhL+8GZVVZV8Pl90KSsrS/RXppSJRQWSuBUCAEgPCQ8Wy5YtU21tbXSprq5O9FemlAnhYb1rGpp1sqHZ4WoAABiYhN8K8Xq98nq9if6alJXv9ajUl6NDtU3ac7xes/OHO10SAAD9xjgWgwBDewMA0kXcVyzq6+u1e/fu6Pt9+/bp7bff1vDhwzV27Fhbi8sUE4uH6LVdJ/T3I3VOlwIAwIDEfcVi8+bNmjlzpmbOnClJWrJkiWbOnKnvfve7theXKc4ZO0yS9Js3D+iNPTUOVwMAQP9ZxpikTlLh9/vl8/lUW1urwsLCZH71oBUKGd25aqv+7zuH5cvN0up/Pl8TwgNnAQAwGPT17zfPWAwCLpelH103QzPHDlXtqRZ9+fFN9BABAKQkgsUgkZPl1iMLZ2vMsFztr2nU1/9riwKtQafLAgAgLgSLQaSowKvHbp6jAq9Hf9n/oZY9u01JvlMFAMCAECwGmcmjCvSrm86R22Xp2bc+0K/W7u59IwAABgmCxSB00eQife/qsyRJP/rjTv3PXw85XBEAAH1DsBikbjp3nL56Qbkk6Zu/+6u2HDjpcEUAAPSOYDGILbviTH3yzFFqbg3pa09uVvWHjU6XBABAjwgWg5jbZennN35MZ5UWqqahWV9+fJP8TS1OlwUAQLcIFoNcvtejFYvmaFShV7uO1Wvxb95SSzDkdFkAAHSJYJECSnw5WrFojnKz3Hpt1wnd+/x2uqECAAYlgkWKmHaGT7/4wkxZlvTbNw9qxZ/2OV0SAACnIVikkMunjtJ3rjhTkvT9F9/TmnePOlwRAACxCBYp5isXlOumc8fKGOnOp7bqbx/UOl0SAABRBIsUY1mW7vvsWbpw0kidagnqK09s0pHaJqfLAgBAEsEiJWW5XfrVTedoUvEQHfUH9JUnNqkh0Op0WQAAECxSVWFOlh67eY5GDsnW9kN+3bXqbQVD9BQBADiLYJHCyobn6ZEvzZbX49Ir7x1V1YvvOV0SACDDESxS3Dljh+nH18+QJP3vP+3TrzcecLgiAEAmI1ikgSvPLtX/+lSlJOne57drw87jDlcEAMhUBIs08c+XVOhz54xRMGS0+DdvaefROqdLAgBkIIJFmrAsS1XXTte55cNVF2jVLSs36XhdwOmyAAAZhmCRRrI9Lj30xVkqH5mvDz46pa/912Y1tQSdLgsAkEEIFmlmWH62Hrt5jobmZWnrwY/0rd/9VSG6oQIAkoRgkYbKR+broS/OUpbb0v9957B++spOp0sCAGQIgkWaOm/CCFVde7Yk6Zev7tYzW953uCIAQCYgWKSxz88ao9s/MVGStPTZd1T1h/d0rI55RQAAiUOwSHNLLp+sqz9Wqpag0cPr9+qCH6zVd1Zv04GaBqdLAwCkIcsYk9Qn+/x+v3w+n2pra1VYWJjMr85YoZDRq38/pgfX7dZbBz+SJLmstoG1br24QlNLOQ8AgJ719e83wSKDGGP0l30f6sF1e7S+w+icn6gs0m2XTNTHy4c7WB0AYDAjWKBH2w/Vavm6PXpx22FFeqPOHjdMt11SoUunFMuyLGcLBAAMKgQL9Mn+Ew16eMNePbPlfTUHQ5KkKSUFuvXiCl159mh53DyGAwAgWCBOx/xNWhGeHbWhuW20zjHDcvX1iyboutllyslyO1whAMBJBAv0S21ji/5r43499uf9+rChWZI0cki2bplXroVzx6kwJ8vhCgEATiBYYEBONQf1fzZX65ENe/XBR6ckSQVej246b5y+fMF4FRfkOFwhACCZCBawRUswpP/56yEtX7dHu47VS2qb7Oz62WP0tQsrNHZEnsMVAgCSgWABW4VCRv8vPBbG1g5jYVw1o20sjDNHcy4BIJ0RLJAQxhi9GR4LY0OHsTBmjRumyaMKVFGUr/KRbUvZ8Dxl0asEANICwQIJ97cParV8fdtYGF39K/K4LI0dnhcNGuVF+ZowcogmFOWruMDLWBkAkEIIFkiagzWN2nzgQ+070aC9Jxq093iD9p2oV1NLqNtt8rLd0cAxoWiIJnQIH/Q8AYDBp69/vz1JrAlpauyIvNMe4gyFjI7WNWnf8diwse9Eg6pPnlJjc1DbD/m1/ZD/tP2NHOKNCRrlI/NVUpijoXlZGpqXrQKvRy4XVzsAYDDiigWSrrk1pOqTjTFho+11g47VBXrd3mVJvtwsDcvLli8vS0Nz2wLH0LwsDc0N/wyHkLbPCCQAMFBcscCgle1xqaJoiCqKhkgaFfNZXVOL9p9o1N5w4IgsJ+oC+uhUixqbgwoZ6WRji042tsT1vd0FEl9ulvK9buVle5SX7Q4vba9zs93K7/A6st7rcfGMCAB0gWCBQaUgJ0vTx/g0fYyvy88DrUHVnmrRR42Rpbnt56nIzw7rIp8PMJB0xWUpJojkZnuUHw0f7tM+83pc0SXb45LX4455nR35PMulbLdL3qyOn7etI8gASAX9ChYPPvigfvjDH+rw4cM666yz9LOf/UwXXnih3bUBp/F63CoucMc98mdvgaSxOajG5lY1Ngd1qjmohuZWnWoOhte3fxZobXsgNWSk+kCr6gOtiTjMLmV3CCcdg0m2xyWPy1KW2xVeLHncbWHE47ai67LcLnlcLmV5LGW52tp63Fa0Xds2VriNS1nhfXrC69wuSx631fbTFfnpin3v7mZ9+CfhCEh/cQeLp59+WnfffbcefPBBzZs3Tw8//LAWLFigd999V2PHjk1EjcCA9TeQdNYaDOlUSyR8tAWOyOtT4fDR8XUkqDS3hhRobQsmba9DXa6LvI+s66g5vK5uQEfgLHeHoBEJG26XJZfV9t4VWWeFX1sd2rgsuS21t3e3/Tytvbt9O5dlyRXexrIsuV0Kr4tsG37vCrezIu3a3rvC+2hb3/7dHdu7wp+1rVf4fXj78E/Laq8lpn3M54rut2N7S+3trU77jHkfaeeyZKl9vTp8V2S95VL76/Dnktq/R+01EQYRr7gf3jz33HN1zjnnaPny5dF1Z555pq655hpVVVX1uj0PbwJ9Y4xRczAUGzxagm3rWkJqDobU1BJUSzCklqBRSzCk1mDbNq3h9y3BkFpDRi2tIbWEIm3a23e5Tbh9ayik5qBRMNT2WTDUtrRGf4ba3wdj14eS+kg4Es2yFBs2wiHGCr/uGHQ6hxIrvL2iAanD9uqwXXfrFbuf9u/usK7Ttoppf/r2iq4//VgiOcrqtI/O6xRTa/jzTvtSp+/v/N8yvJvYbWPadawn9ju7bKP2IPjN+ZNVYHPX/YQ8vNnc3KwtW7Zo6dKlMevnz5+v119/vX+VAuiSZVnhWx6pN2V9KGQUNCYmeLScFkTa3kfaRZaQMQqGFH3dGjJt+wu3DfW0Tfjz6Dam7bNQyChkFPO682fBkJExkW0UXt+2367Xt703kf2E928i32M6fq5O741CodPbmw7tguF01nm9MUZG7cdipOjnHX8axdY0EG37a3sRbFszsB0i4f75ExW2B4u+iitYnDhxQsFgUKNGxT7JP2rUKB05cqTLbQKBgAKB9i6Efv/p4xYASC8ulyWXLGWlXiZKW6aLoCN1CiQKh4hoOOkQVEynEKO2kKXO67pobxRZ181rdQxA7es6ByV1rknt36PT9h1bQ5f7DbdRl3V22Hfn/XdqH/5Y6vzfUO37UjffFdk2eozReju3P72N6dC48+f52c71zejXN3e+52aM6fY+XFVVlf7t3/6tP18DALCJZbU9o9J+IR1IjLhmiBo5cqTcbvdpVyeOHTt22lWMiGXLlqm2tja6VFdX979aAAAwqMUVLLKzszVr1iytWbMmZv2aNWt0/vnnd7mN1+tVYWFhzAIAANJT3LdClixZooULF2r27NmaO3euHnnkER08eFC33nprIuoDAAApJO5gccMNN6impkbf+973dPjwYU2bNk0vvviixo0bl4j6AABACmESMgAA0Ku+/v2O6xkLAACAnhAsAACAbQgWAADANgQLAABgG4IFAACwDcECAADYhmABAABsQ7AAAAC2Sfq8qpHxuJg+HQCA1BH5u93buJpJDxZ1dXWSpLKysmR/NQAAGKC6ujr5fL5uP0/6kN6hUEiHDh1SQUGBLMuybb9+v19lZWWqrq5O+6HCM+lYpcw6Xo41fWXS8XKs6ckYo7q6OpWWlsrl6v5JiqRfsXC5XBozZkzC9p9JU7Nn0rFKmXW8HGv6yqTj5VjTT09XKiJ4eBMAANiGYAEAAGyTNsHC6/Xq3nvvldfrdbqUhMukY5Uy63g51vSVScfLsWa2pD+8CQAA0lfaXLEAAADOI1gAAADbECwAAIBtCBYAAMA2KRUsHnzwQZWXlysnJ0ezZs3Sa6+91mP79evXa9asWcrJydGECRP00EMPJanS/quqqtKcOXNUUFCg4uJiXXPNNdqxY0eP26xbt06WZZ22/P3vf09S1f133333nVZ3SUlJj9uk4nmVpPHjx3d5nhYvXtxl+1Q7rxs2bNBVV12l0tJSWZal5557LuZzY4zuu+8+lZaWKjc3V5dccom2b9/e636feeYZTZ06VV6vV1OnTtXq1asTdAR919OxtrS06J577tH06dOVn5+v0tJSfelLX9KhQ4d63Ofjjz/e5fluampK8NH0rLfzevPNN59W83nnndfrfgfjeZV6P96uzpFlWfrhD3/Y7T4H67lNlJQJFk8//bTuvvtufec739HWrVt14YUXasGCBTp48GCX7fft26crrrhCF154obZu3apvf/vbuvPOO/XMM88kufL4rF+/XosXL9bGjRu1Zs0atba2av78+WpoaOh12x07dujw4cPRZdKkSUmoeODOOuusmLq3bdvWbdtUPa+StGnTppjjXLNmjSTpuuuu63G7VDmvDQ0NmjFjhh544IEuP//P//xP/eQnP9EDDzygTZs2qaSkRJdffnl0/qCuvPHGG7rhhhu0cOFC/fWvf9XChQt1/fXX680330zUYfRJT8fa2Niot956S//6r/+qt956S88++6x27typz372s73ut7CwMOZcHz58WDk5OYk4hD7r7bxK0qc//emYml988cUe9zlYz6vU+/F2Pj+PPfaYLMvS5z73uR73OxjPbcKYFPHxj3/c3HrrrTHrpkyZYpYuXdpl+3/5l38xU6ZMiVn39a9/3Zx33nkJqzERjh07ZiSZ9evXd9tm7dq1RpI5efJk8gqzyb333mtmzJjR5/bpcl6NMeauu+4yFRUVJhQKdfl5Kp9XSWb16tXR96FQyJSUlJj7778/uq6pqcn4fD7z0EMPdbuf66+/3nz605+OWfepT33K3HjjjbbX3F+dj7Urf/nLX4wkc+DAgW7brFy50vh8PnuLs1lXx7po0SJz9dVXx7WfVDivxvTt3F599dXm0ksv7bFNKpxbO6XEFYvm5mZt2bJF8+fPj1k/f/58vf76611u88Ybb5zW/lOf+pQ2b96slpaWhNVqt9raWknS8OHDe207c+ZMjR49WpdddpnWrl2b6NJss2vXLpWWlqq8vFw33nij9u7d223bdDmvzc3N+vWvf60vf/nLvU7Gl6rntaN9+/bpyJEjMefO6/Xq4osv7vZ3WOr+fPe0zWBUW1sry7I0dOjQHtvV19dr3LhxGjNmjK688kpt3bo1OQUO0Lp161RcXKzJkyfrn/7pn3Ts2LEe26fLeT169KheeOEFfeUrX+m1baqe2/5IiWBx4sQJBYNBjRo1Kmb9qFGjdOTIkS63OXLkSJftW1tbdeLEiYTVaidjjJYsWaILLrhA06ZN67bd6NGj9cgjj+iZZ57Rs88+q8rKSl122WXasGFDEqvtn3PPPVdPPvmkXn75ZT366KM6cuSIzj//fNXU1HTZPh3OqyQ999xz+uijj3TzzTd32yaVz2tnkd/TeH6HI9vFu81g09TUpKVLl+of//Efe5ykasqUKXr88cf1/PPP66mnnlJOTo7mzZunXbt2JbHa+C1YsEC/+c1v9Oqrr+rHP/6xNm3apEsvvVSBQKDbbdLhvErSE088oYKCAl177bU9tkvVc9tfSZ/ddCA6/5+dMabH/9vrqn1X6wer22+/Xe+8847+9Kc/9diusrJSlZWV0fdz585VdXW1fvSjH+miiy5KdJkDsmDBgujr6dOna+7cuaqoqNATTzyhJUuWdLlNqp9XSVqxYoUWLFig0tLSbtuk8nntTry/w/3dZrBoaWnRjTfeqFAopAcffLDHtuedd17MQ4/z5s3TOeeco1/+8pf6xS9+kehS++2GG26Ivp42bZpmz56tcePG6YUXXujxD24qn9eIxx57TDfddFOvz0qk6rntr5S4YjFy5Ei53e7T0uyxY8dOS70RJSUlXbb3eDwaMWJEwmq1yx133KHnn39ea9eu7dc08+edd15KpuH8/HxNnz6929pT/bxK0oEDB/TKK6/oq1/9atzbpup5jfT0ied3OLJdvNsMFi0tLbr++uu1b98+rVmzJu4ptV0ul+bMmZNy53v06NEaN25cj3Wn8nmNeO2117Rjx45+/R6n6rntq5QIFtnZ2Zo1a1b0KfqINWvW6Pzzz+9ym7lz557W/o9//KNmz56trKyshNU6UMYY3X777Xr22Wf16quvqry8vF/72bp1q0aPHm1zdYkXCAT03nvvdVt7qp7XjlauXKni4mJ95jOfiXvbVD2v5eXlKikpiTl3zc3NWr9+fbe/w1L357unbQaDSKjYtWuXXnnllX6FXmOM3n777ZQ73zU1Naquru6x7lQ9rx2tWLFCs2bN0owZM+LeNlXPbZ859dRovFatWmWysrLMihUrzLvvvmvuvvtuk5+fb/bv32+MMWbp0qVm4cKF0fZ79+41eXl55hvf+IZ59913zYoVK0xWVpb57//+b6cOoU9uu+024/P5zLp168zhw4ejS2NjY7RN52P96U9/alavXm127txp/va3v5mlS5caSeaZZ55x4hDi8s1vftOsW7fO7N2712zcuNFceeWVpqCgIO3Oa0QwGDRjx44199xzz2mfpfp5raurM1u3bjVbt241ksxPfvITs3Xr1mhPiPvvv9/4fD7z7LPPmm3btpkvfOELZvTo0cbv90f3sXDhwpieXn/+85+N2+02999/v3nvvffM/fffbzwej9m4cWPSj6+jno61paXFfPaznzVjxowxb7/9dszvcSAQiO6j87Hed9995qWXXjJ79uwxW7duNbfccovxeDzmzTffdOIQo3o61rq6OvPNb37TvP7662bfvn1m7dq1Zu7cueaMM85IyfNqTO//jo0xpra21uTl5Znly5d3uY9UObeJkjLBwhhjfvWrX5lx48aZ7Oxsc84558R0wVy0aJG5+OKLY9qvW7fOzJw502RnZ5vx48d3+49gMJHU5bJy5cpom87H+oMf/MBUVFSYnJwcM2zYMHPBBReYF154IfnF98MNN9xgRo8ebbKyskxpaam59tprzfbt26Ofp8t5jXj55ZeNJLNjx47TPkv18xrpHtt5WbRokTGmrcvpvffea0pKSozX6zUXXXSR2bZtW8w+Lr744mj7iN/97nemsrLSZGVlmSlTpgyKYNXTse7bt6/b3+O1a9dG99H5WO+++24zduxYk52dbYqKisz8+fPN66+/nvyD66SnY21sbDTz5883RUVFJisry4wdO9YsWrTIHDx4MGYfqXJejen937Exxjz88MMmNzfXfPTRR13uI1XObaIwbToAALBNSjxjAQAAUgPBAgAA2IZgAQAAbEOwAAAAtiFYAAAA2xAsAACAbQgWAADANgQLAABgG4IFAACwDcECAADYhmABAABsQ7AAAAC2+f9QQoHnvUhaQAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(loss_t[:], '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Equations Derivations\n",
    "Let us consider a simple 2-layer perceptron with input $x_1 \\in \\mathbb{R}^{N_1 \\times 1}$ and some non-linear activation functions. We have the following equations, along with their shapes:\n",
    "$$a_1 = W_1 x_1 + b_1, \\hspace{10pt} W_1 \\in \\mathbb{R}^{M_1 \\times N_1}  \\hspace{5pt} b_1, a_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$ h_1 = f_1(a_1), \\hspace{10pt} h_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    "$$a_2 = W_2 h_1 + b_2, \\hspace{10pt} W_2 \\in \\mathbb{R}^{M_2 \\times M_1}  \\hspace{5pt} b_2, a_2 \\in \\mathbb{R}^{M_2 \\times 1} $$\n",
    " $$ y_2 = f_2(a_2), \\hspace{10pt} y_2 \\in \\mathbb{R}^{M_2 \\times 1} $$\n",
    " $$L = \\frac{1}{2}(y_2 - y_g)^2 \\hspace{10pt} L \\in \\mathbb{R}^{M_2 \\times 1}$$\n",
    "\n",
    "\n",
    " Now, we consider the back-propagation step, starting with the first derivatives:\n",
    " $$\\frac{\\partial L} {\\partial y_2} = y_2 - y_g$$\n",
    " $$\\frac{\\partial L} {\\partial a_2} = \\frac{\\partial L} {\\partial y_2} \\cdot \\frac{\\partial y_2} {\\partial y_2} =  (y_2 - y_g) \\odot f_2'(a_2)$$\n",
    "  $$\\boxed{\\frac{\\partial L} {\\partial W_2} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial W_2} =   (y_2 - y_g) \\odot f_2'(a_2) \\times h_1^T}$$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_2} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial b_2} =   (y_2 - y_g) \\odot f_2'(a_2) \\times 1}$$\n",
    "$$\\frac{\\partial L} {\\partial h_1} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial h_1} =  W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right)  $$\n",
    "$$\\frac{\\partial L} {\\partial a_1} = \\frac{\\partial L} {\\partial h_1} \\cdot \\frac{\\partial h_1} {\\partial a_1} =    W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1) $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial W_1} = \\frac{\\partial L} {\\partial a_1} \\cdot \\frac{\\partial a_1} {\\partial W_1} =    W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1) \\times x_1^T} $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_1} = \\frac{\\partial L} {\\partial a_1} \\cdot \\frac{\\partial a_1} {\\partial b_1} =  W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1)}$$\n",
    "\n",
    "\n",
    "Now, let us do this generically for an n-layer neural network:\n",
    "$$a_1 = W_1 x_1 + b_1, \\hspace{10pt} W_1 \\in \\mathbb{R}^{M_1 \\times N_1}  \\hspace{5pt} b_1, a_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$ h_1 = f_1(a_1), \\hspace{10pt} h_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$a_j = W_j h_{j-1} + b_j, \\hspace{10pt} W_j \\in \\mathbb{R}^{M_{j} \\times M_{j-1}}  \\hspace{5pt} b_j, a_j \\in \\mathbb{R}^{M_j \\times 1} $$\n",
    " $$ h_j = f_j(a_j), \\hspace{10pt} y_j \\in \\mathbb{R}^{M_j \\times 1} $$\n",
    "$$a_N = W_N h_{N-1} + b_N, \\hspace{10pt} W_n \\in \\mathbb{R}^{M_{N} \\times M_{N-1}}  \\hspace{5pt} b_N, a_N \\in \\mathbb{R}^{M_N \\times 1} $$\n",
    " $$ y_N = f_N(a_N), \\hspace{10pt} y_N \\in \\mathbb{R}^{M_N \\times 1} $$\n",
    " $$L = (y_N - y_g)^2 \\hspace{10pt} L \\in \\mathbb{R}^{M_N \\times 1}$$\n",
    "\n",
    " $$\\boxed{\\frac{\\partial L} {\\partial W_j} = \\frac{\\partial L} {\\partial a_j} \\cdot \\frac{\\partial a_j} {\\partial W_j} =   \\left(W_{j-1}^T \\times \\left(...W_N \\times (y_N - y_g) \\odot f_N'(a_N)\\right)... \\odot f_{N-1}'(a_{N-1}) \\right) \\odot f_j'(a_j) \\times h_{j-1}^T} $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_j} = \\frac{\\partial L} {\\partial a_j} \\cdot \\frac{\\partial a_j} {\\partial b_j} =   \\left(W_{j-1}^T \\times \\left(...W_N \\times (y_N - y_g) \\odot f_N'(a_N)\\right)... \\odot f_{N-1}'(a_{N-1}) \\right) \\odot f_j'(a_j)}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, input):\n",
    "        self.tensor = np.array(input)\n",
    "        self.size = self.tensor.shape\n",
    "\n",
    "\n",
    "class LayerT:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.biases = np.zeros(shape=(nout, 1))\n",
    "        self.weights = np.random.uniform(-np.sqrt(6/(nin + nout)), np.sqrt(6/(nin + nout)), size=(nout, nin))\n",
    "        self.weights_grad = 0.0 * self.weights\n",
    "        self.biases_grad = 0.0 * self.biases\n",
    "        self.x = None\n",
    "        \n",
    "    def _broadcast_rule_checker(self, x, y):\n",
    "        x_shape = x.shape\n",
    "        y_shape = y.shape\n",
    "\n",
    "        for i in range(1, min(len(x_shape), len(y_shape))+1):\n",
    "            if x_shape[-i] != y_shape[-i]: \n",
    "                raise ValueError(f\"Dimensions do not match: dim {-i}\")\n",
    "        \n",
    "        while len(x_shape) < len(y_shape):\n",
    "            x = np.expand_dims(x, 0)\n",
    "        while len(y_shape) < len(x_shape):\n",
    "            y = np.expand_dims(y, 0)\n",
    "        return x,y\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return self.weights @ x + self.biases\n",
    "        \n",
    "    def parameters_count(self):\n",
    "        return self.weights.size + self.biases.size\n",
    "    \n",
    "    def _backward(self):\n",
    "        return \n",
    "        \n",
    "class NonLinearity(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def fun(self, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod \n",
    "    def _backward(self):\n",
    "        pass\n",
    "\n",
    "class Tanh(NonLinearity):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = None\n",
    "        \n",
    "    def fun(self, x):\n",
    "        self.x = x\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def __call__(self, x):\n",
    "         return self.fun(x)\n",
    "    \n",
    "    def _backward(self):\n",
    "        return (1- self.fun(self.x)**2)\n",
    "    \n",
    "class Relu(NonLinearity):\n",
    "    def __init__(self, alpha=0.0):\n",
    "        self.alpha = alpha\n",
    "        self.x = None\n",
    "            \n",
    "    def fun(self, x):\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.fun(x)\n",
    "        \n",
    "    def _backward(self):\n",
    "         return np.where(self.x > 0, 1.0, self.alpha)\n",
    "        \n",
    "\n",
    "class Sigmoid(NonLinearity):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = None\n",
    "\n",
    "    def fun(self, x):\n",
    "        self.x = x\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.fun(x)\n",
    "    \n",
    "    def _backward(self):\n",
    "        return self.fun(self.x) * (1-self.fun(self.x))\n",
    "    \n",
    "class NormLoss:\n",
    "    def __init__(self, pow=2):\n",
    "        self.pow = pow\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return np.sum(np.abs(x-y)**self.pow)\n",
    "    \n",
    "    def _backward(self):\n",
    "        if self.x is None:\n",
    "            raise ValueError(\"No Forward Pass Called\")\n",
    "        return 2 * (self.x-self.y)\n",
    "    \n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers=[]):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, l):\n",
    "        l = [l] if not isinstance(l, list) else l\n",
    "        self.layers.extend(l)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = np.array(x).reshape(-1, 1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, loss : NormLoss):\n",
    "        acc_grad = loss._backward()\n",
    "        temp_grad = None\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, NonLinearity):\n",
    "                acc_grad = acc_grad * layer._backward()\n",
    "            elif isinstance(layer, LayerT):\n",
    "                temp_grad = acc_grad\n",
    "                # print(\"bias matrix grad shape:\", temp_grad.shape)\n",
    "                # print(\"layer x:\", layer.x.T.shape)\n",
    "                acc_grad = temp_grad @ layer.x.T\n",
    "                # print(\"post h multipy shape:\", acc_grad.shape)\n",
    "                layer.weights +=  -0.1 * acc_grad\n",
    "                layer.biases += -0.1 * temp_grad\n",
    "                # print(layer.weights_grad.shape == layer.weights.shape)\n",
    "                # print(layer.biases_grad.shape == layer.biases.shape)\n",
    "                acc_grad = layer.weights.T @ temp_grad\n",
    "                # print(\"post W multipy shape:\", acc_grad.shape)\n",
    "        return acc_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8278533743966844"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = LayerT(3, 10)\n",
    "s1 = Sigmoid()\n",
    "s2 = Sigmoid()\n",
    "s3 = Sigmoid()\n",
    "s4 = Sigmoid()\n",
    "b = LayerT(10, 20)\n",
    "c = LayerT(20, 5)\n",
    "d = LayerT(5, 1)\n",
    "e = Tanh()\n",
    "l = NormLoss()\n",
    "nn = NeuralNetwork([a, s1, b,s2, c, s3, d, e])\n",
    "\n",
    "xs_t = xs[0].copy()\n",
    "l(nn(xs_t), ys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(500):\n",
    "    nn.backward(l)\n",
    "    l(nn(xs_t), ys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98214748]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn(xs_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003187124170043572"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "l(nn(xs_t), ys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias matrix grad shape: (1, 1)\n",
      "layer x: (1, 5)\n",
      "post h multipy shape: (1, 5)\n",
      "True\n",
      "True\n",
      "post W multipy shape: (5, 1)\n",
      "bias matrix grad shape: (5, 1)\n",
      "layer x: (1, 20)\n",
      "post h multipy shape: (5, 20)\n",
      "True\n",
      "True\n",
      "post W multipy shape: (20, 1)\n",
      "bias matrix grad shape: (20, 1)\n",
      "layer x: (1, 10)\n",
      "post h multipy shape: (20, 10)\n",
      "True\n",
      "True\n",
      "post W multipy shape: (10, 1)\n",
      "bias matrix grad shape: (10, 1)\n",
      "layer x: (1, 3)\n",
      "post h multipy shape: (10, 3)\n",
      "True\n",
      "True\n",
      "post W multipy shape: (3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.0020129 ],\n",
       "       [ 0.00592731],\n",
       "       [-0.00608339]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.backward(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
