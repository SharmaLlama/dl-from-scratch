{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    def __init__(self, value, op=None, children=()):\n",
    "        self.value = value\n",
    "        self.grad = 0\n",
    "        self.is_leaf = not children\n",
    "        self.children = children\n",
    "        self.op = op\n",
    "        self._backward = lambda : None\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Scalar) else Scalar(other)\n",
    "        out = Scalar(self.value + other.value, children=(self, other), op='+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "        \n",
    "    def __sub__(self, other):\n",
    "        other = other if isinstance(other, Scalar) else Scalar(other)\n",
    "        out = Scalar(self.value - other.value, op='-',  children=(self, other))\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad -= 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Scalar) else Scalar(other)\n",
    "        out = Scalar(self.value * other.value, op='*',  children=(self, other))\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return other * self\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other ** -1)\n",
    "            \n",
    "    def __pow__(self, other):\n",
    "        out = Scalar(self.value ** other, children=(self, ), op='power')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * self.value ** (other - 1) * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v.children:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "        # self.grad = 1\n",
    "        # self._backward()\n",
    "        # child_list  = [c for c in self.children]\n",
    "        # while child_list:\n",
    "        #     curr = child_list.pop(0)\n",
    "        #     curr._backward()\n",
    "        #     if not curr.is_leaf:\n",
    "        #         for d in curr.children:\n",
    "        #             child_list.append(d)\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.value\n",
    "        out = Scalar(np.exp(x), children=(self, ), op='exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * out.value\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.value\n",
    "        fun = lambda x: (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "        out = Scalar(fun(x), children=(self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 - fun(x)**2)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        x = self.value\n",
    "        fun = lambda x: 1/(1 + np.exp(-x))\n",
    "        out = Scalar(fun(x), children=(self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * fun(x) * (1-fun(x))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        x = self.value\n",
    "        out = Scalar(np.max([0, x]), children=(self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 if x > 0 else 0\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def leaky_relu(self, alpha=0.1):\n",
    "        x = self.value\n",
    "        out = Scalar(np.max([x, -alpha*x]), children=(self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 if x > 0 else alpha\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Scalar(data: {self.value})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Scalar(2.1)\n",
    "b = Scalar(0.8)\n",
    "c = a * b\n",
    "d = c * 0.25\n",
    "e = d.tanh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Scalar(data: 2.1),\n",
       " Scalar(data: 0.8),\n",
       " Scalar(data: 1.6800000000000002),\n",
       " Scalar(data: 0.42000000000000004),\n",
       " Scalar(data: 0.39693043200507755))"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c,d,e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Scalar(x) for x in np.random.normal(size=nin)]\n",
    "        self.b = Scalar(0.0)\n",
    "\n",
    "    \n",
    "    def __call__(self, input, activation='relu', args=None):\n",
    "        assert len(input) == len(self.w), \"input size is not equal to weights initialisation\"\n",
    "        out = sum((wi * xi for wi, xi in zip(self.w, input)), self.b)\n",
    "        if activation == 'relu':\n",
    "            post_act = out.relu()\n",
    "        elif activation == 'leaky_relu':\n",
    "            alpha = 0.1 if args is None else args \n",
    "            post_act = out.leaky_relu(alpha=alpha)\n",
    "        elif activation == 'sigmoid':\n",
    "            post_act = out.sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            post_act = out.tanh()\n",
    "        else: \n",
    "            raise ValueError(\"Incorrect Activation Function provided\")\n",
    "        \n",
    "        return post_act\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "    def parameters_count(self):\n",
    "        return len(self.w) + 1\n",
    "    \n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, lsize, nin):\n",
    "        self.neurons = [Neuron(nin) for _ in range(lsize)]\n",
    "\n",
    "    def __call__(self, *args, activation='relu'):\n",
    "        layer_out = [neuron(args[0], activation=activation) for neuron in self.neurons]\n",
    "        return layer_out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "    \n",
    "    def parameters_count(self):\n",
    "        return sum(x.parameters_count() for x in self.neurons)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, lsizes, nin):\n",
    "        sz = [nin] + lsizes\n",
    "        self.layers = [Layer(sz[i+1], sz[i]) for i in range(len(lsizes))]\n",
    "\n",
    "    def __call__(self, *args, activation='relu'):\n",
    "        x = args[0]\n",
    "        for layer in self.layers:\n",
    "            x  = layer(x, activation=activation)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "    def parameters_count(self):\n",
    "        return sum(x.parameters_count() for x in self.layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0], \n",
    "    [3.0, -1, 0.5], \n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1]\n",
    "]\n",
    "\n",
    "ys = [1, -1, -1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP([4, 4, 1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_t = []\n",
    "for _ in range(20):\n",
    "    ypred = [model(x, activation='tanh')[0] for x in xs]\n",
    "    loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "    for p in model.parameters():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.value += -0.05 * p.grad\n",
    "    \n",
    "    loss_t.append(loss.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Scalar(data: 0.8483610396224256),\n",
       " Scalar(data: -0.8188368351386829),\n",
       " Scalar(data: -0.8274681586473587),\n",
       " Scalar(data: 0.8212747397934365)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13ff43a30>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqu0lEQVR4nO3de3TU9Z3/8dfcMrkwCdfcTLhIFRSQyqUV8bbaoghWf/an2HWRatutW2yl9nSF3e2q7Wmj29b1tK5YXUtxXa21YI8/sVqsgFrFckmVioIWhCCEyC33TDIzn98fk5nMhCRkJt+5Px/nzMnMdz7fmc/XLzl5+fl8P++vzRhjBAAAYAF7qjsAAACyB8ECAABYhmABAAAsQ7AAAACWIVgAAADLECwAAIBlCBYAAMAyBAsAAGAZZ7K/MBAI6ODBg/J4PLLZbMn+egAAEAdjjJqbm1VZWSm7vf9xiaQHi4MHD6q6ujrZXwsAACxQV1enqqqqft9PerDweDySgh0rLi5O9tcDAIA4NDU1qbq6Ovx3vD9JDxah6Y/i4mKCBQAAGeZUlzFw8SYAALAMwQIAAFiGYAEAACxDsAAAAJYhWAAAAMsQLAAAgGUIFgAAwDIECwAAYBmCBQAAsAzBAgAAWIZgAQAALEOwAAAAlsmKYGGM0c/++IG++8zbamzvSnV3AADIWVkRLGw2mx5/c5+e2XZAdcfaUt0dAAByVlYEC0mqGlEgSTpwnGABAECqZGGwaE9xTwAAyF1ZFCwKJREsAABIpSwKFkyFAACQalkYLBixAAAgVbIoWPRMhRhjUtwbAAByUxYFi+CIRYvXRy0LAABSJGuCRb7LoTEetySmQwAASJWsCRYSF3ACAJBqWRYsWHIKAEAqZVmwYGUIAACplKXBgqkQAABSIcuCBVMhAACkUpYFi56pEGpZAACQfFkVLE4bTi0LAABSKaZg4fP59G//9m+aMGGCCgoKdPrpp+v73/++AoFAovoXE2pZAACQWs5YGt933316+OGHtXr1ak2ZMkVbt27VzTffrJKSEt1+++2J6mNMqkYU6JNmrw4cb9PU00pS3R0AAHJKTMHizTff1NVXX60FCxZIksaPH6+nnnpKW7duTUjn4lE1olC1+08wYgEAQArENBVywQUX6I9//KN2794tSXr77bf1+uuv68orr+x3H6/Xq6ampqhHIlHLAgCA1IlpxOLOO+9UY2OjJk+eLIfDIb/frx/+8If60pe+1O8+NTU1uueee4bc0cGilgUAAKkT04jF008/rSeeeEJPPvmktm/frtWrV+snP/mJVq9e3e8+K1asUGNjY/hRV1c35E4PhFoWAACkTkwjFt/97ne1fPly3XDDDZKkadOmad++faqpqdGSJUv63Mftdsvtdg+9p4MUGrGoO9YmY4xsNlvSvhsAgFwX04hFW1ub7PboXRwOR9osN5V6alm0dvp1oo1aFgAAJFNMIxZXXXWVfvjDH2rs2LGaMmWKamtrdf/99+uWW25JVP9iFqplEVxy2q4RRXmp7hIAADkjpmDx85//XN/73vf0jW98Qw0NDaqsrNTXv/51/fu//3ui+heXyFoW06qoZQEAQLLEFCw8Ho8eeOABPfDAAwnqjjWoZQEAQGpk1b1CQlhyCgBAamRlsKhmySkAACmRlcGC6psAAKRGlgeLYC0LAACQHFkZLCqpZQEAQEpkZbDIdzlU6glW+2Q6BACA5MnKYCGxMgQAgFTI4mDByhAAAJIti4MFIxYAACRbFgcLRiwAAEi2LA4W1LIAACDZciBYUMsCAIBkydpgQS0LAACSL2uDBbUsAABIvqwNFhIrQwAASLYsDxbBlSF1BAsAAJIiy4MFK0MAAEimLA8W1LIAACCZsjxYcI0FAADJlCPBop1aFgAAJEFWB4tQLYu2Tr+OU8sCAICEy+pgke9yqKw4VMuC6RAAABItq4OFxAWcAAAkUw4ECy7gBAAgWXIoWDBiAQBAouVAsGAqBACAZMmBYMFUCAAAyZIDwaJnxIJaFgAAJFbWB4vK4fmSqGUBAEAyZH2wcDupZQEAQLJkfbCQuIATAIBkyZFgwQWcAAAkQ44FC0YsAABIpBwJFkyFAACQDDkSLJgKAQAgGXIkWARHLOqOUcsCAIBEyolgEapl0d7l17HWzhT3BgCA7JUTwSK6lgXXWQAAkCg5ESwkLuAEACAZcihYcAEnAACJloPBghELAAASJYeCRWgqhBELAAASJWeCRTXXWAAAkHA5Eywip0KoZQEAQGLkTLCoGJ4vm41aFgAAJFLOBAu306EyT7BQFtMhAAAkRs4EC4mVIQAAJFqOBgtWhgAAkAg5FixYGQIAQCLlWLBgxAIAgETKsWDBiAUAAImUY8GCWhYAACRSTgULalkAAJBYORUsqGUBAEBi5VSwkKhlAQBAIuVwsGBlCAAAVsvBYBFcGVJHsAAAwHI5GCyYCgEAIFFyMFhQywIAgETJwWDRc40FtSwAALBWzgWLUC2Ljq6AjlLLAgAAS+VcsKCWBQAAiZNzwUKSqkey5BQAgETIyWDBBZwAACRGjgYLRiwAAEiEHA8WjFgAAGClHA0WTIUAAJAIORosqGUBAEAi5GSwqCgpoJYFAAAJEHOw+Pjjj/UP//APGjVqlAoLC/XpT39a27ZtS0TfEibPaVd5MbUsAACwWkzB4vjx45o7d65cLpd+//vfa+fOnfrpT3+q4cOHJ6h7icPKEAAArOeMpfF9992n6upqrVq1Krxt/PjxVvcpKapGFGrLR8cZsQAAwEIxjVg899xzmjVrlq677jqVlpbq3HPP1aOPPpqoviUUIxYAAFgvpmCxZ88erVy5UmeccYZeeukl3XrrrfrWt76lxx9/vN99vF6vmpqaoh7pgFoWAABYL6apkEAgoFmzZulHP/qRJOncc8/Vu+++q5UrV+qmm27qc5+amhrdc889Q++pxahlAQCA9WIasaioqNDZZ58dte2ss87S/v37+91nxYoVamxsDD/q6uri66nFqGUBAID1YhqxmDt3rnbt2hW1bffu3Ro3bly/+7jdbrnd7vh6l0CRtSyOtHRqjCf9+ggAQKaJacTi29/+tjZv3qwf/ehH+vDDD/Xkk0/qkUce0dKlSxPVv4SJrmXBBZwAAFghpmAxe/ZsPfvss3rqqac0depU/eAHP9ADDzygG2+8MVH9Sygu4AQAwFoxTYVI0sKFC7Vw4cJE9CXpqGUBAIC1cvJeISHUsgAAwFoECzEVAgCAVXI6WFSHa1kwYgEAgBVyOlhEFsmilgUAAEOX08GivCRfdpvk9QVrWQAAgKHJ6WBBLQsAAKyV08FC4p4hAABYiWDByhAAACxDsKCWBQAAliFYMBUCAIBlCBaMWAAAYBmCBbUsAACwTM4HC2pZAABgnZwPFtSyAADAOjkfLCQu4AQAwCoEC1HLAgAAqxAs1BMs6pgKAQBgSAgWYioEAACrECxELQsAAKxCsFDPiMXH1LIAAGBICBaKrmXxSYs31d0BACBjESzUu5YF11kAABAvgkU3LuAEAGDoCBbdqkZyAScAAENFsOjGiAUAAENHsOhG9U0AAIaOYNGNWhYAAAwdwaJbNbUsAAAYMoJFN2pZAAAwdASLbi6HXRUlXGcBAMBQECwinMYFnAAADAnBIgIXcAIAMDQEiwjUsgAAYGgIFhGoZQEAwNAQLCIwFQIAwNAQLCJQywIAgKEhWESglgUAAENDsIgQWcui7hjXWQAAECuCRS+ncZ0FAABxI1j0wsoQAADiR7DohVoWAADEj2DRC0tOAQCIH8Gil1Cw+JgRCwAAYkaw6CVUy+LAiXYFAtSyAAAgFgSLXipK8uWw29TpC+gItSwAAIgJwaIXp8Ou8uJ8SVId0yEAAMSEYNEHLuAEACA+BIs+sOQUAID4ECz6QJEsAADiQ7DoA1MhAADEh2DRh6qI26cDAIDBI1j0ITxiQS0LAABiQrDoA7UsAACID8GiD9SyAAAgPgSLfnABJwAAsSNY9INaFgAAxI5g0Q9qWQAAEDuCRT+YCgEAIHYEi34wFQIAQOwIFv0IjVh8fJxaFgAADBbBoh/hWhb+gD6hlgUAAINCsOhHZC0LrrMAAGBwCBYDYGUIAACxIVgMoHokF3ACABALgsUAWHIKAEBsCBYDYMkpAACxIVgMgGssAACIDcFiANSyAAAgNgSLAZQXU8sCAIBYECwG4HTYVVFCLQsAAAZrSMGipqZGNptNy5Yts6g76YfrLAAAGLy4g8WWLVv0yCOP6JxzzrGyP2mHlSEAAAxeXMGipaVFN954ox599FGNGDHC6j6lFWpZAAAweHEFi6VLl2rBggX63Oc+d8q2Xq9XTU1NUY9MwogFAACD54x1h1//+tfavn27tmzZMqj2NTU1uueee2LuWLrgGgsAAAYvphGLuro63X777XriiSeUn58/qH1WrFihxsbG8KOuri6ujqYKtSwAABi8mEYstm3bpoaGBs2cOTO8ze/369VXX9WDDz4or9crh8MRtY/b7Zbb7bamtynQu5ZFWfHgAhUAALkopmBx2WWXaceOHVHbbr75Zk2ePFl33nnnSaEiG4RqWRw43q66Y20ECwAABhBTsPB4PJo6dWrUtqKiIo0aNeqk7dmkakSBDhxv14Hj7Zo1PtW9AQAgfVF5cxB6Voaw5BQAgIHEvCqkt40bN1rQjfTGyhAAAAaHEYtBoJYFAACDQ7AYhNCIxb5jrSnuCQAA6Y1gMQiTyz3Kc9pVd6xdWz86luruAACQtggWgzC8ME9fnHGaJOmRV/ekuDcAAKQvgsUgfeWC0yVJ6987rD2ftKS4NwAApCeCxSB9qnSYPndWqYyR/vv1vanuDgAAaYlgEYN/vGiiJGnNtgM60uJNcW8AAEg/BIsYzB4/QtOrh8vrC+jxN/elujsAAKQdgkUMbDab/vHC4LUW//PmR2rv9Ke4RwAApBeCRYyumFqu6pEFOt7Wpd9uP5Dq7gAAkFYIFjFy2G36avcKkf9+bY/8AZPiHgEAkD4IFnG4blaVhhe6tO9om9bvrE91dwAASBsEizgU5jm1+LxxkiiYBQBAJIJFnG6aM155Tru27z9BmW8AALoRLOI0xuOmzDcAAL0QLIaAMt8AAEQjWAwBZb4BAIhGsBgiynwDANCDYDFElPkGAKAHwWKIKPMNAEAPgoUFKPMNAEAQwcIClPkGACCIYGERynwDAECwsAxlvgEAIFhY6qY545XnoMw3ACB3ESwsNMbj1rWU+QYA5DCChcW+eiFlvgEAuYtgYTHKfAMAchnBIgEo8w0AyFUEiwSgzDcAIFcRLBKAMt8AgFxFsEgQynwDAHIRwSJBKPMNAMhFBIsEosw3ACDXECwSiDLfAIBcQ7BIMMp8AwByCcEiwSjzDQDIJQSLJKDMNwAgVxAskoAy3wCAXEGwSJKvdY9aUOYbAJDNCBZJ8pkJIynzDQDIegSLJKHMNwAgFxAskogy3wCAbEewSCLKfAMAsh3BIsko8w0AyGYEiySjzDcAIJsRLFKAMt8AgGxFsEgBynwDALIVwSJFKPMNAMhGBIsUocw3ACAbESxSiDLfAIBsQ7BIIcp8AwCyDcEihSjzDQDINgSLFLt8SllPme9tdanuDgAAQ0KwSDGnw65b5k6QJD2zjfuHAAAyG8EiDVw1vVJ2m/TOgUbtP9qW6u4AABA3gkUaGD3MrTkTR0mSnt9xMMW9AQAgfgSLNLHwnEpJ0rp3DqW4JwAAxI9gkSYun1Iuh92mdw82ae+R1lR3BwCAuBAs0sTIojzN/dRoSdK6d5gOAQBkJoJFGlk4rUKS9DzTIQCADEWwSCOXTymXy2HT+/XN+rCBG5MBADIPwSKNlBS6dEF4OoRRCwBA5iFYpJnQ6pDnuc4CAJCBCBZp5vNTypTnsOuDhhbtPtyc6u4AABATgkWaKc536aIzx0iSnn+bUQsAQGYhWKShhed0rw7ZcUjGmBT3BgCAwSNYpKHLzipVntOuPZ+06r1DTIcAADIHwSINefJd+rtJwemQddw7BACQQWIKFjU1NZo9e7Y8Ho9KS0t1zTXXaNeuXYnqW05bEF4dwnQIACBzxBQsNm3apKVLl2rz5s1av369fD6f5s2bp9ZW7m1htcsmlyrfZde+o21692BTqrsDAMCgOGNp/OKLL0a9XrVqlUpLS7Vt2zZddNFFlnYs1xW5nbp0cqle2FGv//fOQU09rSTVXQIA4JSGdI1FY2OjJGnkyJH9tvF6vWpqaop6YHAib6XOdAgAIBPEHSyMMbrjjjt0wQUXaOrUqf22q6mpUUlJSfhRXV0d71fmnL+bVKrCPIcOHG/XOwcaU90dAABOKe5gcdttt+mdd97RU089NWC7FStWqLGxMfyoq6uL9ytzTkGeQ5edVSaJEt8AgMwQV7D45je/qeeee04bNmxQVVXVgG3dbreKi4ujHhi8Bd23Umc6BACQCWIKFsYY3XbbbVq7dq1eeeUVTZgwIVH9QrdLJo1RUZ5DBxs7tH3/iVR3BwCAAcUULJYuXaonnnhCTz75pDwej+rr61VfX6/29vZE9S/n5bsc+vzZwekQbqUOAEh3MQWLlStXqrGxUZdccokqKirCj6effjpR/YN6Voe8sOOQAgGmQwAA6SumOhbM8afGhWeOliffqfqmDm3bf1yzx/e/vBcAgFTiXiEZwO10aN7Z5ZK4lToAIL0RLDJE6FbqL/y1Xn6mQwAAaYpgkSHmfmq0Sgpc+qTZqz/vPZbq7gAA0CeCRYbIc9p1+ZTu1SHcSh0AkKYIFhkkdCv13++ol88fSHFvAAA4GcEig5w/cZRGFLp0tLVTbzEdAgBIQwSLDOJy2HXF1O7VIRTLAgCkIYJFhgkVy3rxr4fUxXQIACDNECwyzGcnjNSoojwdb+vSm387muruAAAQhWCRYZwOu+ZPC02HsDoEAJBeCBYZaMG04HTIS+8eVqeP6RAAQPogWGSgz0wYqTEetxrbu/SnD4+kujsAAIQRLDKQw27TlawOAQCkIYJFhlo4PTgd8oed9fL6/CnuDQAAQQSLDDVz7AiVFbvV3OHTa7uZDgEApAeCRYay2226clrwjqesDgEApAuCRQYLFctav/OwOrqYDgEApB7BIoOdWz1clSX5au30a+OuT1LdHQAACBaZzG63acE5wemQdTtYHQIASD2CRYYL3Ur9j+8dVnsn0yEAgNQiWGS46VUlqhpRoLZOvzbsakh1dwAAOY5gkeFstojpEIplAQBSjGCRBa4KTYe8f1itXl+KewMAyGUEiywwpbJY40YVqqMroFfeZzoEAJA6BIssYLPZtPAcimUBAFKPYJElQrdS37DrE7UwHQIASBGCRZY4q8Kj08cUqdMX0Ms7D6e6OwCAHEWwyBI2m00Lw/cOYXUIACA1CBZZJFQs69Xdn6ixvSvFvQEA5CKCRRaZVO7RGaXD1OlnOgQAkBoEiyyzgNUhAIAUIlhkmdCy09c+OKLGNqZDAADJRbDIMp8q9WhyuUe+gNFLO+tT3R0AQI4hWGShnmJZrA4BACQXwSILhVaH/OnDIzre2pni3gAAcgnBIgtNGF2kKZXF8geMXnyX6RAAQPIQLLIUt1IHAKQCwSJLLey+d8gbfzuiIy3eFPcGAJArCBZZauyoQp1TVaKAkV78K9MhAIDkIFhkMW6lDgBINoJFFruy+6Zkb+09pobmjhT3BgCQCwgWWaxqRKE+XT1cxki/38F0CAAg8QgWWW4hq0MAAElEsMhyoemQLfuOaceBxhT3BgCQ7QgWWa5yeIE+M36kjJGuevB1/Z+H/qSn/rxfzR3coAwAYD2bMcYk8wubmppUUlKixsZGFRcXJ/Orc1bdsTb94PmdeuX9BvkCwdNd4HJowTkVun5WtWaPHyGbzZbiXgIA0tlg/34TLHLIJ81ePVt7QE9vqdPfPmkNb58wukjXzarS/51RpdLi/BT2EACQrggW6JcxRtv3n9BvttTp+XcOqrXTL0ly2G265Mwxun52tS6dXCqXg5kyAEAQwQKD0ur1ad2OQ/rNljpt3Xc8vH30sDxdO6NK18+q0qdKPSnsIQAgHRAsELMPG1r0zLY6rdn2cdT9RWaMHa5Fs6u14JxKDXM7U9hDAECqECwQty5/QBveb9Bvth7Qhl0N8ndf8FmY59CCaRVaNLtaM8dxwScA5BKCBSzR0NShNds/1jNb67TnSM8Fn6ePKdL1s6p17YzTVOrhgk8AyHYEC1jKGKOt+453X/B5SO1dPRd8/t2kUl01vUJnVRRrwugiLvoEgCxEsEDCtHh9ev7tg/rN1jpt338i6j2Xw6bTRw/TmeUeTSobpjPLPDqzzKPqkYVy2Jk6AYBMRbBAUnxwuFm/3XZAWz46pt2HW9Ti9fXZLt9l1xmlwZAxqXxY90+PyovzuVYDADIAwQJJZ4zRwcYO7a5v1q7DzeGfHza0yOsL9LmPJ9+pSWWe7hEOj84oG6ZJZR6NGuZOcu8BAAMhWCBt+ANG+4+1aVd9s3Yf7gkde460hlec9DZ6WF54GmVSuUdjRxaqrDhfFSX5KmLJKwAkHcECac/r82vvkVbtqm/WB4dbgoHjcLP2H2vTQP8qPW6nykryVV6cHw4bodflxfkqK3FrdJFbdq7pAADLDPbvN//rh5RxOx2aXF6syeXR/0DbOn36sKElPMKx+3CLPj7RrsONHWr2+oKPhhZ92NDS72c77TaVFeerrNit8pKIANIdPkLb8l2ORB8mAOQUggXSTmGeU+dUDdc5VcNPeq/V61N9U4fqG7sfTR063NTzvL6xQ5+0eOULGH18ol0fn2gf8LuGF7pUXpyvEYV5GlHk0vDCPI0odGlEYZ5KClxR24cXuFRS4JKT5bQA0C+CBTJKkdupiWOGaeKYYf228fkD+qTFGxU+6ps6dDgifNQ3daijK6ATbV060dYVUx+K850aUZQXDhsjCkOBJE/DC10a3h1MQq9LCl0aludkagZATiBYIOs4HXZVlBSooqSg3zbGGDW1+8IjHsfbOnW8tVMn2oNB43hbp463delEW6eOt3XqRFuXmjuCS2mbOnxq6vBp39G2mPo1zO1UkduhYW6nhuW75HE7u593/4x47um1zeN2BffNd8rtZPoGQPoiWCAn2Ww2lXSPJkwqH9zdW7v8ATW2h8JGTwA5ERlCWrt0or0zKpx0di+1bfH61OL16bC8p/imgeU57FFhpMjtUEGeUwUuuwrznCrIc6jA5VBhnqPXc2c/20PPnRQxAzBkBAtgkFwOu0YPc2t0DDU2jDHq6AqoxetTa3ewaO7wdYeMLrV0BC9GbfX6ws9bwu9HP2/rDJZR7/QHdKy1U8daOy0/xjyn/aTAke9yKN9lV77TIXfET3ev1/nOYFu30x7986Q2Pfu6nXYKpAFZhmABJJDNZgv+gc5zaIxnaEW/fP6AWjv9EYEjOD3T1ulXe6dfbV1+tXf61N4ZUFuXT+1R23u16d7W1ulXe5c/vLy30xdQpy84MpMseQ678pzdj36euyNeu3q1cfe1b+RrR3Afl9Mul90W/Omwy+WwRb/nsMllj37OdTFA7AgWQIZwOuwqKbCrpMBl6ecaY+T1BcIhIxxOOn1q6/LL2+WX1xdQR+TProA6fH51dAXkDf/s1cYXiNo38r3Iumid/oA6/QENcYYoIZx2WziEuMIhJPg8z2GX02GT0x5832kPvnY57OH9Qu877bao95yO3vtEfM5J+9vksEc+D77nsAdfhz4j/Lp7f4c9GI4cjlC74DZGiJBoBAsgx9lstu7pjuRcFGqMkS9gwkEjNErS6Q/+9PZ6HXzuDz/39n6v12tvr/d8gYA6/UZdvoC6/KGHiXre2f28d2E2X8DIF/AriQM4CecIhw6b7PaI4NK9PRRQ7BFhJHJ7z+vgPnZb9/ZQ8LFF7+OIbBPxufbe7SJeh7/bFtH+pM9Wz2d0twu+7tke+R3Rbfto0709cpvdZpPdJsJYjAgWAJLKZrOFRwAGd9ls8vgDwcDR6Q90BxFzUhjp9Afk8xt1+gLqCgSf+/wBdQWCP31+E97e5Q8Ew0n3/r7w9u7nUfuc3K7npwn3zd/9PLxvr/d8gZ72/R2jP2Bk/RU62ctmU0R46X7e/drRvS0qjISCS8Q+dltEYIncx2YLfn73vpHPQ8EmtJ+t1/Oefii6vd2mOz5/pjz51o5uDhbBAgC6Bf/vNnmjN4lkjIkOGpFhpjuUhEKG3xj5/D3tA1GvA71e934/uL8/IuQEX3f/7GNbINDrvYDkDwTkNwq+F/E9PW169u39XsAYBUL7dm/vea5w29C2gDEKBBRue+r/lpLPGGkQbdPFNy75lDz5qfnuuILFQw89pB//+Mc6dOiQpkyZogceeEAXXnih1X0DAMTJZgte10HZk1OLCiThn+oziAR6tzERr0MhJtxOEcEn8vODwS8ciLrfMxHtjVHPd3f3pc/nkd9lTPhzC/NSd+JjDhZPP/20li1bpoceekhz587VL37xC82fP187d+7U2LFjE9FHAAASxm63yS6bsmCgKi3EfHfTz372s5oxY4ZWrlwZ3nbWWWfpmmuuUU1NzSn35+6mAABknsH+/Y7pbkqdnZ3atm2b5s2bF7V93rx5euONN/rcx+v1qqmpKeoBAACyU0zB4siRI/L7/SorK4vaXlZWpvr6+j73qampUUlJSfhRXV0df28BAEBai+v+z73X9Bpj+l3nu2LFCjU2NoYfdXV18XwlAADIADFdvDl69Gg5HI6TRicaGhpOGsUIcbvdcruHVsoYAABkhphGLPLy8jRz5kytX78+avv69et1/vnnW9oxAACQeWJebnrHHXdo8eLFmjVrlubMmaNHHnlE+/fv16233pqI/gEAgAwSc7BYtGiRjh49qu9///s6dOiQpk6dqhdeeEHjxo1LRP8AAEAGibmOxVBRxwIAgMyTkDoWAAAAAyFYAAAAyxAsAACAZQgWAADAMnHdNn0oQteKcs8QAAAyR+jv9qnWfCQ9WDQ3N0sS9wwBACADNTc3q6SkpN/3k77cNBAI6ODBg/J4PP3eXyQeTU1Nqq6uVl1dXdYvY82lY5Vy63g51uyVS8fLsWYnY4yam5tVWVkpu73/KymSPmJht9tVVVWVsM8vLi7O+pMbkkvHKuXW8XKs2SuXjpdjzT4DjVSEcPEmAACwDMECAABYJmuChdvt1l133ZUTt2jPpWOVcut4OdbslUvHy7HmtqRfvAkAALJX1oxYAACA1CNYAAAAyxAsAACAZQgWAADAMhkVLB566CFNmDBB+fn5mjlzpl577bUB22/atEkzZ85Ufn6+Tj/9dD388MNJ6mn8ampqNHv2bHk8HpWWluqaa67Rrl27Btxn48aNstlsJz3ef//9JPU6fnffffdJ/S4vLx9wn0w8r5I0fvz4Ps/T0qVL+2yfaef11Vdf1VVXXaXKykrZbDb97ne/i3rfGKO7775blZWVKigo0CWXXKJ33333lJ+7Zs0anX322XK73Tr77LP17LPPJugIBm+gY+3q6tKdd96padOmqaioSJWVlbrpppt08ODBAT/zV7/6VZ/nu6OjI8FHM7BTndcvf/nLJ/X5vPPOO+XnpuN5lU59vH2dI5vNph//+Mf9fma6nttEyZhg8fTTT2vZsmX613/9V9XW1urCCy/U/PnztX///j7b7927V1deeaUuvPBC1dbW6l/+5V/0rW99S2vWrElyz2OzadMmLV26VJs3b9b69evl8/k0b948tba2nnLfXbt26dChQ+HHGWeckYQeD92UKVOi+r1jx45+22bqeZWkLVu2RB3n+vXrJUnXXXfdgPtlynltbW3V9OnT9eCDD/b5/n/8x3/o/vvv14MPPqgtW7aovLxcn//858P3D+rLm2++qUWLFmnx4sV6++23tXjxYl1//fV66623EnUYgzLQsba1tWn79u363ve+p+3bt2vt2rXavXu3vvCFL5zyc4uLi6PO9aFDh5Sfn5+IQxi0U51XSbriiiui+vzCCy8M+Jnpel6lUx9v7/Pzy1/+UjabTV/84hcH/Nx0PLcJYzLEZz7zGXPrrbdGbZs8ebJZvnx5n+3/+Z//2UyePDlq29e//nVz3nnnJayPidDQ0GAkmU2bNvXbZsOGDUaSOX78ePI6ZpG77rrLTJ8+fdDts+W8GmPM7bffbiZOnGgCgUCf72fyeZVknn322fDrQCBgysvLzb333hve1tHRYUpKSszDDz/c7+dcf/315oorrojadvnll5sbbrjB8j7Hq/ex9uXPf/6zkWT27dvXb5tVq1aZkpISaztnsb6OdcmSJebqq6+O6XMy4bwaM7hze/XVV5tLL710wDaZcG6tlBEjFp2dndq2bZvmzZsXtX3evHl64403+tznzTffPKn95Zdfrq1bt6qrqythfbVaY2OjJGnkyJGnbHvuueeqoqJCl112mTZs2JDorlnmgw8+UGVlpSZMmKAbbrhBe/bs6bdttpzXzs5OPfHEE7rllltOeTO+TD2vkfbu3av6+vqoc+d2u3XxxRf3+zss9X++B9onHTU2Nspms2n48OEDtmtpadG4ceNUVVWlhQsXqra2NjkdHKKNGzeqtLRUZ555pr72ta+poaFhwPbZcl4PHz6sdevW6Stf+cop22bquY1HRgSLI0eOyO/3q6ysLGp7WVmZ6uvr+9ynvr6+z/Y+n09HjhxJWF+tZIzRHXfcoQsuuEBTp07tt11FRYUeeeQRrVmzRmvXrtWkSZN02WWX6dVXX01ib+Pz2c9+Vo8//rheeuklPfroo6qvr9f555+vo0eP9tk+G86rJP3ud7/TiRMn9OUvf7nfNpl8XnsL/Z7G8jsc2i/WfdJNR0eHli9frr//+78f8CZVkydP1q9+9Ss999xzeuqpp5Sfn6+5c+fqgw8+SGJvYzd//nz97//+r1555RX99Kc/1ZYtW3TppZfK6/X2u082nFdJWr16tTwej6699toB22XquY1X0u9uOhS9/8/OGDPg/+311b6v7enqtttu0zvvvKPXX399wHaTJk3SpEmTwq/nzJmjuro6/eQnP9FFF12U6G4Oyfz588PPp02bpjlz5mjixIlavXq17rjjjj73yfTzKkmPPfaY5s+fr8rKyn7bZPJ57U+sv8Px7pMuurq6dMMNNygQCOihhx4asO15550XddHj3LlzNWPGDP385z/Xz372s0R3NW6LFi0KP586dapmzZqlcePGad26dQP+wc3k8xryy1/+UjfeeOMpr5XI1HMbr4wYsRg9erQcDsdJabahoeGk1BtSXl7eZ3un06lRo0YlrK9W+eY3v6nnnntOGzZsiOs28+edd15GpuGioiJNmzat375n+nmVpH379unll1/WV7/61Zj3zdTzGlrpE8vvcGi/WPdJF11dXbr++uu1d+9erV+/PuZbatvtds2ePTvjzndFRYXGjRs3YL8z+byGvPbaa9q1a1dcv8eZem4HKyOCRV5enmbOnBm+ij5k/fr1Ov/88/vcZ86cOSe1/8Mf/qBZs2bJ5XIlrK9DZYzRbbfdprVr1+qVV17RhAkT4vqc2tpaVVRUWNy7xPN6vXrvvff67XumntdIq1atUmlpqRYsWBDzvpl6XidMmKDy8vKoc9fZ2alNmzb1+zss9X++B9onHYRCxQcffKCXX345rtBrjNFf/vKXjDvfR48eVV1d3YD9ztTzGumxxx7TzJkzNX369Jj3zdRzO2ipumo0Vr/+9a+Ny+Uyjz32mNm5c6dZtmyZKSoqMh999JExxpjly5ebxYsXh9vv2bPHFBYWmm9/+9tm586d5rHHHjMul8v89re/TdUhDMo//dM/mZKSErNx40Zz6NCh8KOtrS3cpvex/ud//qd59tlnze7du81f//pXs3z5ciPJrFmzJhWHEJPvfOc7ZuPGjWbPnj1m8+bNZuHChcbj8WTdeQ3x+/1m7Nix5s477zzpvUw/r83Nzaa2ttbU1tYaSeb+++83tbW14ZUQ9957rykpKTFr1641O3bsMF/60pdMRUWFaWpqCn/G4sWLo1Z6/elPfzIOh8Pce++95r333jP33nuvcTqdZvPmzUk/vkgDHWtXV5f5whe+YKqqqsxf/vKXqN9jr9cb/ozex3r33XebF1980fztb38ztbW15uabbzZOp9O89dZbqTjEsIGOtbm52XznO98xb7zxhtm7d6/ZsGGDmTNnjjnttNMy8rwac+p/x8YY09jYaAoLC83KlSv7/IxMObeJkjHBwhhj/uu//suMGzfO5OXlmRkzZkQtwVyyZIm5+OKLo9pv3LjRnHvuuSYvL8+MHz++338E6URSn49Vq1aF2/Q+1vvuu89MnDjR5OfnmxEjRpgLLrjArFu3Lvmdj8OiRYtMRUWFcblcprKy0lx77bXm3XffDb+fLec15KWXXjKSzK5du056L9PPa2h5bO/HkiVLjDHBJad33XWXKS8vN26321x00UVmx44dUZ9x8cUXh9uHPPPMM2bSpEnG5XKZyZMnp0WwGuhY9+7d2+/v8YYNG8Kf0ftYly1bZsaOHWvy8vLMmDFjzLx588wbb7yR/IPrZaBjbWtrM/PmzTNjxowxLpfLjB071ixZssTs378/6jMy5bwac+p/x8YY84tf/MIUFBSYEydO9PkZmXJuE4XbpgMAAMtkxDUWAAAgMxAsAACAZQgWAADAMgQLAABgGYIFAACwDMECAABYhmABAAAsQ7AAAACWIVgAAADLECwAAIBlCBYAAMAyBAsAAGCZ/w/fR/XK3ExT4AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(loss_t[:], '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Equations Derivations\n",
    "Let us consider a simple 2-layer perceptron with input $x_1 \\in \\mathbb{R}^{N_1 \\times 1}$ and some non-linear activation functions. We have the following equations, along with their shapes:\n",
    "$$a_1 = W_1 x_1 + b_1, \\hspace{10pt} W_1 \\in \\mathbb{R}^{M_1 \\times N_1}  \\hspace{5pt} b_1, a_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$ h_1 = f_1(a_1), \\hspace{10pt} h_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    "$$a_2 = W_2 h_1 + b_2, \\hspace{10pt} W_2 \\in \\mathbb{R}^{M_2 \\times M_1}  \\hspace{5pt} b_2, a_2 \\in \\mathbb{R}^{M_2 \\times 1} $$\n",
    " $$ y_2 = f_2(a_2), \\hspace{10pt} y_2 \\in \\mathbb{R}^{M_2 \\times 1} $$\n",
    " $$L = \\frac{1}{2}(y_2 - y_g)^2 \\hspace{10pt} L \\in \\mathbb{R}^{M_2 \\times 1}$$\n",
    "\n",
    "\n",
    " Now, we consider the back-propagation step, starting with the first derivatives:\n",
    " $$\\frac{\\partial L} {\\partial y_2} = y_2 - y_g$$\n",
    " $$\\frac{\\partial L} {\\partial a_2} = \\frac{\\partial L} {\\partial y_2} \\cdot \\frac{\\partial y_2} {\\partial y_2} =  (y_2 - y_g) \\odot f_2'(a_2)$$\n",
    "  $$\\boxed{\\frac{\\partial L} {\\partial W_2} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial W_2} =   (y_2 - y_g) \\odot f_2'(a_2) \\times h_1^T}$$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_2} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial b_2} =   (y_2 - y_g) \\odot f_2'(a_2) \\times 1}$$\n",
    "$$\\frac{\\partial L} {\\partial h_1} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial h_1} =  W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right)  $$\n",
    "$$\\frac{\\partial L} {\\partial a_1} = \\frac{\\partial L} {\\partial h_1} \\cdot \\frac{\\partial h_1} {\\partial a_1} =    W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1) $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial W_1} = \\frac{\\partial L} {\\partial a_1} \\cdot \\frac{\\partial a_1} {\\partial W_1} =    W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1) \\times x_1^T} $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_1} = \\frac{\\partial L} {\\partial a_1} \\cdot \\frac{\\partial a_1} {\\partial b_1} =  W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1)}$$\n",
    "\n",
    "\n",
    "Now, let us do this generically for an n-layer neural network:\n",
    "$$a_1 = W_1 x_1 + b_1, \\hspace{10pt} W_1 \\in \\mathbb{R}^{M_1 \\times N_1}  \\hspace{5pt} b_1, a_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$ h_1 = f_1(a_1), \\hspace{10pt} h_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$a_j = W_j h_{j-1} + b_j, \\hspace{10pt} W_j \\in \\mathbb{R}^{M_{j} \\times M_{j-1}}  \\hspace{5pt} b_j, a_j \\in \\mathbb{R}^{M_j \\times 1} $$\n",
    " $$ h_j = f_j(a_j), \\hspace{10pt} y_j \\in \\mathbb{R}^{M_j \\times 1} $$\n",
    "$$a_N = W_N h_{N-1} + b_N, \\hspace{10pt} W_n \\in \\mathbb{R}^{M_{N} \\times M_{N-1}}  \\hspace{5pt} b_N, a_N \\in \\mathbb{R}^{M_N \\times 1} $$\n",
    " $$ y_N = f_N(a_N), \\hspace{10pt} y_N \\in \\mathbb{R}^{M_N \\times 1} $$\n",
    " $$L = (y_N - y_g)^2 \\hspace{10pt} L \\in \\mathbb{R}^{M_N \\times 1}$$\n",
    "\n",
    " $$\\boxed{\\frac{\\partial L} {\\partial W_j} = \\frac{\\partial L} {\\partial a_j} \\cdot \\frac{\\partial a_j} {\\partial W_j} =   \\left(W_{j-1}^T \\times \\left(...W_N \\times (y_N - y_g) \\odot f_N'(a_N)\\right)... \\odot f_{N-1}'(a_{N-1}) \\right) \\odot f_j'(a_j) \\times h_{j-1}^T} $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_j} = \\frac{\\partial L} {\\partial a_j} \\cdot \\frac{\\partial a_j} {\\partial b_j} =   \\left(W_{j-1}^T \\times \\left(...W_N \\times (y_N - y_g) \\odot f_N'(a_N)\\right)... \\odot f_{N-1}'(a_{N-1}) \\right) \\odot f_j'(a_j)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Equations Derivations (With Batches)\n",
    "Now, let us consider a simple 2-layer perceptron with input $x_1 \\in \\mathbb{R}^{B \\times N_1 \\times 1}$ and some non-linear activation functions. We have the following equations, along with their shapes:\n",
    "$$a_1 = W_1 x_1 + b_1, \\hspace{10pt} W_1 \\in \\mathbb{R}^{M_1 \\times N_1}  \\hspace{5pt} b_1, a_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$ h_1 = f_1(a_1), \\hspace{10pt} h_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    "$$a_2 = W_2 h_1 + b_2, \\hspace{10pt} W_2 \\in \\mathbb{R}^{M_2 \\times M_1}  \\hspace{5pt} b_2, a_2 \\in \\mathbb{R}^{M_2 \\times 1} $$\n",
    " $$ y_2 = f_2(a_2), \\hspace{10pt} y_2 \\in \\mathbb{R}^{M_2 \\times 1} $$\n",
    " $$L = \\frac{1}{2}(y_2 - y_g)^2 \\hspace{10pt} L \\in \\mathbb{R}^{M_2 \\times 1}$$\n",
    "\n",
    "\n",
    " Now, we consider the back-propagation step, starting with the first derivatives:\n",
    " $$\\frac{\\partial L} {\\partial y_2} = y_2 - y_g$$\n",
    " $$\\frac{\\partial L} {\\partial a_2} = \\frac{\\partial L} {\\partial y_2} \\cdot \\frac{\\partial y_2} {\\partial y_2} =  (y_2 - y_g) \\odot f_2'(a_2)$$\n",
    "  $$\\boxed{\\frac{\\partial L} {\\partial W_2} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial W_2} =   (y_2 - y_g) \\odot f_2'(a_2) \\times h_1^T}$$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_2} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial b_2} =   (y_2 - y_g) \\odot f_2'(a_2) \\times 1}$$\n",
    "$$\\frac{\\partial L} {\\partial h_1} = \\frac{\\partial L} {\\partial a_2} \\cdot \\frac{\\partial a_2} {\\partial h_1} =  W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right)  $$\n",
    "$$\\frac{\\partial L} {\\partial a_1} = \\frac{\\partial L} {\\partial h_1} \\cdot \\frac{\\partial h_1} {\\partial a_1} =    W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1) $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial W_1} = \\frac{\\partial L} {\\partial a_1} \\cdot \\frac{\\partial a_1} {\\partial W_1} =    W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1) \\times x_1^T} $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_1} = \\frac{\\partial L} {\\partial a_1} \\cdot \\frac{\\partial a_1} {\\partial b_1} =  W_2^T \\times \\left((y_2 - y_g) \\odot f_2'(a_2)\\right) \\odot f_1'(a_1)}$$\n",
    "\n",
    "\n",
    "Now, let us do this generically for an n-layer neural network:\n",
    "$$a_1 = W_1 x_1 + b_1, \\hspace{10pt} W_1 \\in \\mathbb{R}^{M_1 \\times N_1}  \\hspace{5pt} b_1, a_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$ h_1 = f_1(a_1), \\hspace{10pt} h_1 \\in \\mathbb{R}^{M_1 \\times 1} $$\n",
    " $$a_j = W_j h_{j-1} + b_j, \\hspace{10pt} W_j \\in \\mathbb{R}^{M_{j} \\times M_{j-1}}  \\hspace{5pt} b_j, a_j \\in \\mathbb{R}^{M_j \\times 1} $$\n",
    " $$ h_j = f_j(a_j), \\hspace{10pt} y_j \\in \\mathbb{R}^{M_j \\times 1} $$\n",
    "$$a_N = W_N h_{N-1} + b_N, \\hspace{10pt} W_n \\in \\mathbb{R}^{M_{N} \\times M_{N-1}}  \\hspace{5pt} b_N, a_N \\in \\mathbb{R}^{M_N \\times 1} $$\n",
    " $$ y_N = f_N(a_N), \\hspace{10pt} y_N \\in \\mathbb{R}^{M_N \\times 1} $$\n",
    " $$L = (y_N - y_g)^2 \\hspace{10pt} L \\in \\mathbb{R}^{M_N \\times 1}$$\n",
    "\n",
    " $$\\boxed{\\frac{\\partial L} {\\partial W_j} = \\frac{\\partial L} {\\partial a_j} \\cdot \\frac{\\partial a_j} {\\partial W_j} =   \\left(W_{j-1}^T \\times \\left(...W_N \\times (y_N - y_g) \\odot f_N'(a_N)\\right)... \\odot f_{N-1}'(a_{N-1}) \\right) \\odot f_j'(a_j) \\times h_{j-1}^T} $$\n",
    "$$\\boxed{\\frac{\\partial L} {\\partial b_j} = \\frac{\\partial L} {\\partial a_j} \\cdot \\frac{\\partial a_j} {\\partial b_j} =   \\left(W_{j-1}^T \\times \\left(...W_N \\times (y_N - y_g) \\odot f_N'(a_N)\\right)... \\odot f_{N-1}'(a_{N-1}) \\right) \\odot f_j'(a_j)}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, input):\n",
    "        self.tensor = np.array(input)\n",
    "        self.size = self.tensor.shape\n",
    "\n",
    "\n",
    "class LayerT:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.biases = np.zeros(shape=(nout, 1))\n",
    "        self.weights = np.random.uniform(-np.sqrt(6/(nin + nout)), np.sqrt(6/(nin + nout)), size=(nout, nin))\n",
    "        self.weights_grad = 0.0 * self.weights\n",
    "        self.biases_grad = 0.0 * self.biases\n",
    "        self.x = None\n",
    "        \n",
    "    def _broadcast_rule_checker(self, x, y):\n",
    "        x_shape = x.shape\n",
    "        y_shape = y.shape\n",
    "\n",
    "        for i in range(1, min(len(x_shape), len(y_shape))+1):\n",
    "            if x_shape[-i] != y_shape[-i]: \n",
    "                raise ValueError(f\"Dimensions do not match: dim {-i}\")\n",
    "        \n",
    "        while len(x_shape) < len(y_shape):\n",
    "            x = np.expand_dims(x, 0)\n",
    "        while len(y_shape) < len(x_shape):\n",
    "            y = np.expand_dims(y, 0)\n",
    "        return x,y\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return self.weights @ x + self.biases\n",
    "        \n",
    "    def parameters_count(self):\n",
    "        return self.weights.size + self.biases.size\n",
    "    \n",
    "    def _backward(self):\n",
    "        return\n",
    "\n",
    "    def zero(self):\n",
    "        self.weights_grad *= 0.0\n",
    "        self.biases_grad *= 0.0 \n",
    "        \n",
    "class NonLinearity(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def fun(self, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod \n",
    "    def _backward(self):\n",
    "        pass\n",
    "\n",
    "class Tanh(NonLinearity):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = None\n",
    "        \n",
    "    def fun(self, x):\n",
    "        self.x = x\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def __call__(self, x):\n",
    "         return self.fun(x)\n",
    "    \n",
    "    def _backward(self):\n",
    "        return (1- self.fun(self.x)**2)\n",
    "    \n",
    "class Relu(NonLinearity):\n",
    "    def __init__(self, alpha=0.0):\n",
    "        self.alpha = alpha\n",
    "        self.x = None\n",
    "            \n",
    "    def fun(self, x):\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.fun(x)\n",
    "        \n",
    "    def _backward(self):\n",
    "         return np.where(self.x > 0, 1.0, self.alpha)\n",
    "        \n",
    "\n",
    "class Sigmoid(NonLinearity):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = None\n",
    "\n",
    "    def fun(self, x):\n",
    "        self.x = x\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.fun(x)\n",
    "    \n",
    "    def _backward(self):\n",
    "        return self.fun(self.x) * (1-self.fun(self.x))\n",
    "    \n",
    "class NormLoss:\n",
    "    def __init__(self, pow=2):\n",
    "        self.pow = pow\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return 1/self.pow * np.mean(np.abs(x-y)**self.pow)\n",
    "    \n",
    "    def _backward(self):\n",
    "        if self.x is None:\n",
    "            raise ValueError(\"No Forward Pass Called\")\n",
    "        if self.x.ndim > 0: \n",
    "            return (self.pow - 1) * (self.x-self.y)\n",
    "        else: \n",
    "            return (self.pow - 1) * np.mean(self.x - self.y, -1)\n",
    "    \n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers=[]):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, l):\n",
    "        l = [l] if not isinstance(l, list) else l\n",
    "        self.layers.extend(l)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = np.array(x)\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(-1, 1)\n",
    "        else: \n",
    "            x = x.T\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters_count(self):\n",
    "        return sum(x.parameters_count() for x in self.layers if not isinstance(x, NonLinearity))\n",
    "    \n",
    "    def zero(self): \n",
    "        for layer in self.layers: \n",
    "            if isinstance(layer, LayerT):\n",
    "                layer.zero()\n",
    "    \n",
    "    def backward(self, loss : NormLoss):\n",
    "        acc_grad = loss._backward()\n",
    "        temp_grad = None\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, NonLinearity):\n",
    "                acc_grad = acc_grad * layer._backward()\n",
    "            elif isinstance(layer, LayerT):\n",
    "                temp_grad = acc_grad\n",
    "                # print(\"bias matrix grad shape:\", temp_grad.shape)\n",
    "                # print(\"layer x:\", layer.x.T.shape)\n",
    "                acc_grad = temp_grad @ layer.x.T\n",
    "                # print(\"post h multipy shape:\", acc_grad.shape)\n",
    "                \n",
    "                layer.weights +=  -0.1 * acc_grad\n",
    "                layer.biases += -0.1 * np.mean(temp_grad, axis=-1, keepdims=True)\n",
    "                # print(layer.weights_grad.shape == layer.weights.shape)\n",
    "                # print(layer.biases_grad.shape == layer.biases.shape)\n",
    "                acc_grad = layer.weights.T @ temp_grad\n",
    "                # print(\"post W multipy shape:\", acc_grad.shape)\n",
    "        return acc_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 20 is different from 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[265], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m l \u001b[38;5;241m=\u001b[39m NormLoss()\n\u001b[1;32m     11\u001b[0m nn \u001b[38;5;241m=\u001b[39m NeuralNetwork([a, s1, b,s2, c, s3, d, e])\n\u001b[0;32m---> 12\u001b[0m l(\u001b[43mnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m, ys)\n",
      "Cell \u001b[0;32mIn[264], line 134\u001b[0m, in \u001b[0;36mNeuralNetwork.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    132\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 134\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[264], line 31\u001b[0m, in \u001b[0;36mLayerT.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 20 is different from 3)"
     ]
    }
   ],
   "source": [
    "a = LayerT(3, 10)\n",
    "s1 = Sigmoid()\n",
    "s2 = Sigmoid()\n",
    "s3 = Sigmoid()\n",
    "s4 = Sigmoid()\n",
    "b = LayerT(10, 20)\n",
    "c = LayerT(20, 5)\n",
    "d = LayerT(5, 1)\n",
    "e = Tanh()\n",
    "l = NormLoss()\n",
    "nn = NeuralNetwork([a, s1, b,s2, c, s3, d, e])\n",
    "l(nn(xs), ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(500):\n",
    "    nn.backward(l)\n",
    "    l(nn(xs), ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0030450148726526858"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(nn(xs), ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparsion between Backprop Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(x_features, x_samples):\n",
    "    xs = np.random.random(size=(x_samples, x_features)) * 10 - 5\n",
    "    ys = (np.random.rand(x_samples) > 0.5) * 2 - 1\n",
    "    return xs, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNeuralNets:\n",
    "    def __init__(self, xs, depth, max_width=100):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.hidden_layer_sizes = list(np.random.randint(low=2, high=max_width, size=depth))\n",
    "        self.n1 = self.create_basic_nn()\n",
    "        self.n2 = self.create_tensor_nn()\n",
    "        assert self.n1.parameters_count() == self.n2.parameters_count()\n",
    "\n",
    "    def create_basic_nn(self):\n",
    "        return MLP(self.hidden_layer_sizes + [1], nin=self.xs.shape[1])\n",
    "    \n",
    "    def create_tensor_nn(self): \n",
    "        layers = [LayerT(self.xs.shape[1], self.hidden_layer_sizes[0]), Relu()]\n",
    "        for i in range(len(self.hidden_layer_sizes[:-1])):\n",
    "            layers.append(LayerT(self.hidden_layer_sizes[i], self.hidden_layer_sizes[i+1]))\n",
    "            layers.append(Relu())\n",
    "        layers.append(LayerT(self.hidden_layer_sizes[-1], 1))\n",
    "        layers.append(Tanh())\n",
    "        return NeuralNetwork(layers=layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = create_dataset(x_features=20, x_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = RandomNeuralNets(xs=xs, depth=5, max_width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time_ns\n",
    "from tqdm import tqdm\n",
    "def train_model_basic(model, xs, ys, iters, reps):\n",
    "    times = []\n",
    "    for _ in tqdm(range(reps)):\n",
    "        s1 = time_ns()\n",
    "        for _ in range(iters):\n",
    "            ss1 = time_ns()\n",
    "            ypred = [model(x, activation='tanh')[0] for x in xs]\n",
    "            ss2 = time_ns()\n",
    "            loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "            ss3 = time_ns()\n",
    "            for p in model.parameters():\n",
    "                p.grad = 0.0\n",
    "            ss4 = time_ns()\n",
    "            loss.backward()\n",
    "            ss5 = time_ns()\n",
    "            for p in model.parameters():\n",
    "                p.value += -0.01 * p.grad\n",
    "            ss6 = time_ns()\n",
    "            print(f\"forward pass: {(ss2 - ss1) / 10**6}, loss cal: {(ss3 - ss2) / 10**6}, gradient_zeroing = {(ss4 - ss3) / 10**6}, backprop: {(ss5-ss4)/ 10**6}, GD: {(ss6 - ss5) / 10**6}\")\n",
    "            \n",
    "        time_spent = time_ns() - s1\n",
    "        times.append(time_spent/10**9)\n",
    "    return times\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass: 5756.899, loss cal: 1472.64, gradient_zeroing = 0.052, backprop: 176.419, GD: 0.037\n",
      "forward pass: 2806.618, loss cal: 5.885, gradient_zeroing = 0.066, backprop: 190.79, GD: 0.074\n",
      "forward pass: 369.67, loss cal: 6.408, gradient_zeroing = 0.028, backprop: 219.437, GD: 0.04\n",
      "forward pass: 4077.815, loss cal: 4.46, gradient_zeroing = 0.059, backprop: 180.59, GD: 0.046\n",
      "forward pass: 2564.029, loss cal: 5.521, gradient_zeroing = 0.049, backprop: 201.306, GD: 0.057\n",
      "forward pass: 2554.194, loss cal: 4.248, gradient_zeroing = 0.055, backprop: 182.557, GD: 0.065\n",
      "forward pass: 2600.516, loss cal: 4.313, gradient_zeroing = 0.042, backprop: 186.521, GD: 0.058\n",
      "forward pass: 2681.517, loss cal: 5.012, gradient_zeroing = 0.042, backprop: 191.05, GD: 0.049\n",
      "forward pass: 2704.436, loss cal: 3.645, gradient_zeroing = 0.042, backprop: 182.481, GD: 0.267\n",
      "forward pass: 2835.119, loss cal: 4.747, gradient_zeroing = 0.077, backprop: 182.632, GD: 7.684\n",
      "forward pass: 2742.058, loss cal: 5.013, gradient_zeroing = 0.047, backprop: 183.463, GD: 0.059\n",
      "forward pass: 2892.847, loss cal: 4.58, gradient_zeroing = 0.071, backprop: 179.322, GD: 0.044\n",
      "forward pass: 3046.822, loss cal: 6.335, gradient_zeroing = 0.079, backprop: 185.205, GD: 0.041\n",
      "forward pass: 3368.108, loss cal: 7.539, gradient_zeroing = 0.1, backprop: 192.887, GD: 0.064\n",
      "forward pass: 369.65, loss cal: 13.14, gradient_zeroing = 0.036, backprop: 202.893, GD: 0.05\n",
      "forward pass: 4180.451, loss cal: 4.271, gradient_zeroing = 0.063, backprop: 180.325, GD: 0.057\n",
      "forward pass: 2441.972, loss cal: 3.602, gradient_zeroing = 0.034, backprop: 192.471, GD: 0.037\n",
      "forward pass: 2404.392, loss cal: 3.754, gradient_zeroing = 0.033, backprop: 187.035, GD: 0.051\n",
      "forward pass: 2484.218, loss cal: 4.41, gradient_zeroing = 0.045, backprop: 188.95, GD: 0.065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:00<00:00, 60.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass: 2534.926, loss cal: 4.245, gradient_zeroing = 0.133, backprop: 185.755, GD: 0.058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[60.775157]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_basic(ra.n1, xs, ys, iters=20, reps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_tensor(model, xs, ys, iters, reps):\n",
    "    times = []\n",
    "    l = NormLoss()\n",
    "    for _ in tqdm(range(reps)):\n",
    "        s1 = time_ns()\n",
    "        for _ in range(iters):\n",
    "            ss1 = time_ns()\n",
    "            f = model(xs)\n",
    "            ss2 = time_ns()\n",
    "            l(f, ys)\n",
    "            ss3 = time_ns()\n",
    "            model.backward(l)\n",
    "            ss4 = time_ns()\n",
    "            print(f\"forward pass: {(ss2 - ss1) / 10**6}, loss cal: {(ss3-ss2) / 10**6}, backprop + GD: {(ss4 - ss3) / 10**6}\")\n",
    "            # print(f\"loss: {l(f, ys)}\")\n",
    "        time_spent = time_ns() - s1\n",
    "        times.append(time_spent/10**9)\n",
    "    print(l(f, ys))\n",
    "    return times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34332272224991583"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(ra.n2(xs), ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass: 0.185, loss cal: 0.047, backprop + GD: 0.519\n",
      "forward pass: 0.066, loss cal: 0.041, backprop + GD: 0.27\n",
      "forward pass: 0.078, loss cal: 0.054, backprop + GD: 0.212\n",
      "forward pass: 0.13, loss cal: 0.022, backprop + GD: 0.188\n",
      "forward pass: 0.151, loss cal: 0.036, backprop + GD: 0.277\n",
      "forward pass: 0.05, loss cal: 0.022, backprop + GD: 0.097\n",
      "forward pass: 0.028, loss cal: 0.008, backprop + GD: 0.068\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.063\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.007, backprop + GD: 0.061\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.007, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.173\n",
      "forward pass: 0.03, loss cal: 0.01, backprop + GD: 0.076\n",
      "forward pass: 0.025, loss cal: 0.007, backprop + GD: 0.068\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.024, loss cal: 0.007, backprop + GD: 0.065\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.024, loss cal: 0.007, backprop + GD: 0.065\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.067, loss cal: 0.007, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.083\n",
      "forward pass: 0.024, loss cal: 0.022, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.007, backprop + GD: 0.079\n",
      "forward pass: 0.023, loss cal: 0.007, backprop + GD: 0.08\n",
      "forward pass: 0.024, loss cal: 0.007, backprop + GD: 0.082\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.045, loss cal: 0.007, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.007, backprop + GD: 0.08\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.071\n",
      "forward pass: 0.327, loss cal: 0.032, backprop + GD: 0.302\n",
      "forward pass: 0.084, loss cal: 0.027, backprop + GD: 0.249\n",
      "forward pass: 0.065, loss cal: 0.018, backprop + GD: 0.247\n",
      "forward pass: 0.21, loss cal: 0.154, backprop + GD: 0.366\n",
      "forward pass: 0.028, loss cal: 0.007, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.497, loss cal: 0.066, backprop + GD: 0.089\n",
      "forward pass: 0.33, loss cal: 0.009, backprop + GD: 0.119\n",
      "forward pass: 0.154, loss cal: 0.059, backprop + GD: 0.075\n",
      "forward pass: 0.024, loss cal: 0.008, backprop + GD: 0.319\n",
      "forward pass: 0.069, loss cal: 0.008, backprop + GD: 0.07\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.063\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.004, backprop + GD: 0.063\n",
      "forward pass: 0.045, loss cal: 0.013, backprop + GD: 0.079\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.067\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.022, loss cal: 0.007, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.004, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.004, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.004, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.007, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.004, backprop + GD: 0.063\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.05, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.196, loss cal: 0.015, backprop + GD: 2.735\n",
      "forward pass: 0.044, loss cal: 0.012, backprop + GD: 0.071\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.105\n",
      "forward pass: 0.03, loss cal: 0.007, backprop + GD: 0.07\n",
      "forward pass: 0.028, loss cal: 0.007, backprop + GD: 0.071\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 2.469\n",
      "forward pass: 0.044, loss cal: 0.169, backprop + GD: 0.105\n",
      "forward pass: 0.028, loss cal: 0.008, backprop + GD: 0.067\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.063\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.061\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.109\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.171\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.06\n",
      "forward pass: 0.03, loss cal: 0.005, backprop + GD: 0.34\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.071\n",
      "forward pass: 0.023, loss cal: 0.004, backprop + GD: 0.062\n",
      "forward pass: 0.044, loss cal: 0.012, backprop + GD: 0.083\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.133, loss cal: 0.035, backprop + GD: 0.197\n",
      "forward pass: 0.107, loss cal: 0.008, backprop + GD: 0.21\n",
      "forward pass: 0.104, loss cal: 0.034, backprop + GD: 0.096\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.039, loss cal: 0.007, backprop + GD: 0.576\n",
      "forward pass: 0.286, loss cal: 0.035, backprop + GD: 0.428\n",
      "forward pass: 0.066, loss cal: 0.029, backprop + GD: 0.133\n",
      "forward pass: 0.027, loss cal: 0.007, backprop + GD: 0.065\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.105\n",
      "forward pass: 0.026, loss cal: 0.007, backprop + GD: 0.066\n",
      "forward pass: 0.022, loss cal: 0.01, backprop + GD: 0.116\n",
      "forward pass: 0.105, loss cal: 0.035, backprop + GD: 0.211\n",
      "forward pass: 0.178, loss cal: 0.009, backprop + GD: 0.197\n",
      "forward pass: 0.114, loss cal: 0.031, backprop + GD: 0.402\n",
      "forward pass: 0.03, loss cal: 0.009, backprop + GD: 0.067\n",
      "forward pass: 0.025, loss cal: 0.007, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.084, loss cal: 0.025, backprop + GD: 0.604\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.004, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.063\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.15, loss cal: 0.034, backprop + GD: 0.093\n",
      "forward pass: 0.025, loss cal: 0.007, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.007, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.007, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.066\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.145\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.067\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.066\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.066\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.066\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.066\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.066\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.066\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.066\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.09\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.007, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.024, loss cal: 0.004, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.004, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.004, backprop + GD: 0.061\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.007, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.077\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.027, loss cal: 0.005, backprop + GD: 0.066\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.111\n",
      "forward pass: 0.028, loss cal: 0.008, backprop + GD: 0.066\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.175\n",
      "forward pass: 0.026, loss cal: 0.007, backprop + GD: 0.2\n",
      "forward pass: 0.027, loss cal: 0.008, backprop + GD: 0.285\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.14\n",
      "forward pass: 0.051, loss cal: 0.008, backprop + GD: 0.117\n",
      "forward pass: 0.08, loss cal: 0.027, backprop + GD: 0.101\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.062\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.038, loss cal: 0.033, backprop + GD: 0.142\n",
      "forward pass: 0.084, loss cal: 0.023, backprop + GD: 0.179\n",
      "forward pass: 0.101, loss cal: 0.021, backprop + GD: 0.094\n",
      "forward pass: 0.046, loss cal: 0.013, backprop + GD: 0.083\n",
      "forward pass: 0.16, loss cal: 0.021, backprop + GD: 0.301\n",
      "forward pass: 0.032, loss cal: 0.031, backprop + GD: 0.114\n",
      "forward pass: 0.05, loss cal: 0.009, backprop + GD: 0.136\n",
      "forward pass: 0.091, loss cal: 0.008, backprop + GD: 0.103\n",
      "forward pass: 0.024, loss cal: 0.007, backprop + GD: 0.076\n",
      "forward pass: 0.025, loss cal: 0.007, backprop + GD: 0.075\n",
      "forward pass: 0.027, loss cal: 0.007, backprop + GD: 0.08\n",
      "forward pass: 0.026, loss cal: 0.007, backprop + GD: 0.074\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.046, loss cal: 0.007, backprop + GD: 0.066\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.027, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.027, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.027, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.027, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.027, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.102\n",
      "forward pass: 0.032, loss cal: 0.01, backprop + GD: 0.065\n",
      "forward pass: 0.028, loss cal: 0.007, backprop + GD: 0.061\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.068\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.066\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.069\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.027, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.027, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.027, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.028, loss cal: 0.005, backprop + GD: 0.063\n",
      "forward pass: 0.027, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.027, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.027, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.007, backprop + GD: 0.077\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.102\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.058\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.085\n",
      "forward pass: 0.027, loss cal: 0.007, backprop + GD: 0.061\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.067\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.088\n",
      "forward pass: 0.042, loss cal: 0.006, backprop + GD: 0.104\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.075\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.152\n",
      "forward pass: 0.043, loss cal: 0.017, backprop + GD: 0.075\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.027, loss cal: 0.007, backprop + GD: 0.067\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.07\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.072\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.067\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.072\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.058\n",
      "forward pass: 0.022, loss cal: 0.007, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.056\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.089\n",
      "forward pass: 0.026, loss cal: 0.009, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.057\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.072\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.063\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.164\n",
      "forward pass: 0.026, loss cal: 0.007, backprop + GD: 0.117\n",
      "forward pass: 0.039, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.078\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.081\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.077\n",
      "forward pass: 0.028, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.058\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.058\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.036, loss cal: 0.044, backprop + GD: 0.066\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.058\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.072\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.071\n",
      "forward pass: 0.024, loss cal: 0.007, backprop + GD: 0.058\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.028, loss cal: 0.006, backprop + GD: 0.068\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.027, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.27\n",
      "forward pass: 29.868, loss cal: 0.033, backprop + GD: 0.337\n",
      "forward pass: 0.03, loss cal: 0.009, backprop + GD: 0.106\n",
      "forward pass: 0.047, loss cal: 0.009, backprop + GD: 0.068\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.004, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.07\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.027, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.176\n",
      "forward pass: 0.031, loss cal: 0.008, backprop + GD: 0.069\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.257\n",
      "forward pass: 0.082, loss cal: 0.028, backprop + GD: 0.234\n",
      "forward pass: 0.08, loss cal: 0.021, backprop + GD: 0.163\n",
      "forward pass: 0.056, loss cal: 0.016, backprop + GD: 0.181\n",
      "forward pass: 0.057, loss cal: 0.016, backprop + GD: 0.163\n",
      "forward pass: 0.057, loss cal: 0.133, backprop + GD: 0.243\n",
      "forward pass: 0.071, loss cal: 0.021, backprop + GD: 0.195\n",
      "forward pass: 0.056, loss cal: 0.013, backprop + GD: 0.153\n",
      "forward pass: 0.052, loss cal: 0.012, backprop + GD: 0.153\n",
      "forward pass: 0.052, loss cal: 0.011, backprop + GD: 0.151\n",
      "forward pass: 0.051, loss cal: 0.012, backprop + GD: 0.778\n",
      "forward pass: 0.027, loss cal: 0.007, backprop + GD: 0.065\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.061\n",
      "forward pass: 0.045, loss cal: 0.005, backprop + GD: 0.199\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.107\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.142\n",
      "forward pass: 0.035, loss cal: 0.011, backprop + GD: 0.077\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.4\n",
      "forward pass: 0.029, loss cal: 0.007, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.062, backprop + GD: 0.313\n",
      "forward pass: 0.064, loss cal: 0.018, backprop + GD: 0.162\n",
      "forward pass: 0.055, loss cal: 0.014, backprop + GD: 0.252\n",
      "forward pass: 0.06, loss cal: 0.017, backprop + GD: 0.162\n",
      "forward pass: 0.054, loss cal: 0.014, backprop + GD: 0.157\n",
      "forward pass: 0.054, loss cal: 0.013, backprop + GD: 0.155\n",
      "forward pass: 0.053, loss cal: 0.013, backprop + GD: 0.159\n",
      "forward pass: 0.052, loss cal: 0.013, backprop + GD: 0.195\n",
      "forward pass: 0.056, loss cal: 0.015, backprop + GD: 0.181\n",
      "forward pass: 0.031, loss cal: 0.01, backprop + GD: 0.069\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.058\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.067\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.067\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.058\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.066\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.074\n",
      "forward pass: 0.027, loss cal: 0.007, backprop + GD: 0.066\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.078\n",
      "forward pass: 0.026, loss cal: 0.017, backprop + GD: 0.067\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.089\n",
      "forward pass: 0.031, loss cal: 0.007, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.078\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.067\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.013, backprop + GD: 0.067\n",
      "forward pass: 0.026, loss cal: 0.007, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.069\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.09\n",
      "forward pass: 0.028, loss cal: 0.007, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.058\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.066\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.07\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.059\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.057\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.069\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.066\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.066\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.066\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.066\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.072\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.073\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.067\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.066\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.191\n",
      "forward pass: 0.027, loss cal: 0.007, backprop + GD: 0.06\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.028, loss cal: 0.005, backprop + GD: 0.069\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.066\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.067\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.063\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.067\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.058\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.059\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.069\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.127\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.023, loss cal: 0.01, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.058\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.069\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.067\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.067\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.067\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.067\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.09\n",
      "forward pass: 0.038, loss cal: 0.005, backprop + GD: 0.083\n",
      "forward pass: 0.024, loss cal: 0.028, backprop + GD: 0.063\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.078\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.035, loss cal: 0.006, backprop + GD: 0.07\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.063\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.068\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.073\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.09\n",
      "forward pass: 0.026, loss cal: 0.008, backprop + GD: 0.07\n",
      "forward pass: 0.027, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.058\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.058\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.058\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.058\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.068\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.07\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.023, loss cal: 0.004, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.058\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.069\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.14\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.067\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.004, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.07\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.067\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.183\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.058\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.054\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.059\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.054\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.054\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.054\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.054\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.069\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.023, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.063\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.066\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.005, backprop + GD: 0.063\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.065\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.067\n",
      "forward pass: 0.025, loss cal: 0.01, backprop + GD: 0.064\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.064\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.083\n",
      "forward pass: 0.028, loss cal: 0.007, backprop + GD: 0.067\n",
      "forward pass: 0.026, loss cal: 0.007, backprop + GD: 0.066\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.069\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.057\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.067\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.071\n",
      "forward pass: 0.024, loss cal: 0.006, backprop + GD: 0.074\n",
      "forward pass: 0.032, loss cal: 0.007, backprop + GD: 0.066\n",
      "forward pass: 0.027, loss cal: 0.007, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.065\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.061\n",
      "forward pass: 0.025, loss cal: 0.006, backprop + GD: 0.067\n",
      "forward pass: 0.026, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.062\n",
      "forward pass: 0.026, loss cal: 0.006, backprop + GD: 0.059\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.069\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.069\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.068\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.069\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.06\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.058\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.069\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.085\n",
      "forward pass: 0.023, loss cal: 0.007, backprop + GD: 0.055\n",
      "forward pass: 0.021, loss cal: 0.006, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.023, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.058\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.059\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.059\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.064\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.058\n",
      "forward pass: 0.022, loss cal: 0.006, backprop + GD: 0.054\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.063\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.055\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.058\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.058\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.061\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.062\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.054\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.067\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.057\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.059\n",
      "forward pass: 0.022, loss cal: 0.005, backprop + GD: 0.056\n",
      "forward pass: 0.021, loss cal: 0.005, backprop + GD: 0.056\n",
      "0.3428576154715183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.147862]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_tensor(ra.n2, xs, ys, iters=1000, reps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = create_dataset(x_features=20, x_samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth=2, width=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:40<00:00, 70.09s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/var/folders/lx/bw9zm4ts5xqgrty123rxm60w0000gn/T/ipykernel_41656/3075367339.py:56: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
      "/var/folders/lx/bw9zm4ts5xqgrty123rxm60w0000gn/T/ipykernel_41656/3075367339.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
      "100%|██████████| 10/10 [00:00<00:00, 102.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth=2, width=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x107d98fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dl_scratch/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x107d98fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dl_scratch/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x107d98fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dl_scratch/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      " 20%|██        | 2/10 [03:10<12:41, 95.19s/it]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x107d98fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dl_scratch/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "100%|██████████| 10/10 [14:22<00:00, 86.26s/it]\n",
      "100%|██████████| 10/10 [00:00<00:00, 35.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth=2, width=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x107d98fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dl_scratch/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x107d98fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dl_scratch/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x107d98fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/dl_scratch/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "  0%|          | 0/10 [00:25<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[248], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, width=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m neuralnets \u001b[38;5;241m=\u001b[39m RandomNeuralNets(xs, depth\u001b[38;5;241m=\u001b[39md, max_width\u001b[38;5;241m=\u001b[39mw)\n\u001b[0;32m----> 6\u001b[0m times_t1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_basic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneuralnets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m times_t2 \u001b[38;5;241m=\u001b[39m train_model_tensor(neuralnets\u001b[38;5;241m.\u001b[39mn2, xs, ys, iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, reps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      8\u001b[0m mm[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic\u001b[39m\u001b[38;5;124m\"\u001b[39m, d, w, neuralnets\u001b[38;5;241m.\u001b[39mn1\u001b[38;5;241m.\u001b[39mparameters_count())] \u001b[38;5;241m=\u001b[39m times_t1\n",
      "Cell \u001b[0;32mIn[244], line 12\u001b[0m, in \u001b[0;36mtrain_model_basic\u001b[0;34m(model, xs, ys, iters, reps)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m     11\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m     14\u001b[0m     p\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.01\u001b[39m \u001b[38;5;241m*\u001b[39m p\u001b[38;5;241m.\u001b[39mgrad\n",
      "Cell \u001b[0;32mIn[2], line 69\u001b[0m, in \u001b[0;36mScalar.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m             build_topo(child)\n\u001b[1;32m     68\u001b[0m         topo\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[0;32m---> 69\u001b[0m \u001b[43mbuild_topo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(topo):\n",
      "Cell \u001b[0;32mIn[2], line 67\u001b[0m, in \u001b[0;36mScalar.backward.<locals>.build_topo\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     65\u001b[0m visited\u001b[38;5;241m.\u001b[39madd(v)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mbuild_topo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m topo\u001b[38;5;241m.\u001b[39mappend(v)\n",
      "Cell \u001b[0;32mIn[2], line 67\u001b[0m, in \u001b[0;36mScalar.backward.<locals>.build_topo\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     65\u001b[0m visited\u001b[38;5;241m.\u001b[39madd(v)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mbuild_topo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m topo\u001b[38;5;241m.\u001b[39mappend(v)\n",
      "    \u001b[0;31m[... skipping similar frames: Scalar.backward.<locals>.build_topo at line 67 (182 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[2], line 67\u001b[0m, in \u001b[0;36mScalar.backward.<locals>.build_topo\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     65\u001b[0m visited\u001b[38;5;241m.\u001b[39madd(v)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mbuild_topo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m topo\u001b[38;5;241m.\u001b[39mappend(v)\n",
      "Cell \u001b[0;32mIn[2], line 63\u001b[0m, in \u001b[0;36mScalar.backward.<locals>.build_topo\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     61\u001b[0m topo \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     62\u001b[0m visited \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_topo\u001b[39m(v):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited:\n\u001b[1;32m     65\u001b[0m         visited\u001b[38;5;241m.\u001b[39madd(v)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mm = {}\n",
    "for d in range(2, 15):\n",
    "    for w in range(10, 100, 10):\n",
    "        print(f\"depth={d}, width={w}\")\n",
    "        neuralnets = RandomNeuralNets(xs, depth=d, max_width=w)\n",
    "        times_t1 = train_model_basic(neuralnets.n1, xs, ys, iters=20, reps=10)\n",
    "        times_t2 = train_model_tensor(neuralnets.n2, xs, ys, iters=20, reps=10)\n",
    "        mm[(\"basic\", d, w, neuralnets.n1.parameters_count())] = times_t1\n",
    "        mm[(\"tensor\", d, w, neuralnets.n2.parameters_count())] = times_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8729000 ns\n",
      "3620000 ns\n",
      "4016000 ns\n",
      "3742000 ns\n",
      "2627000 ns\n",
      "2550000 ns\n",
      "2889000 ns\n",
      "2251000 ns\n",
      "2158000 ns\n",
      "2210000 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.008729,\n",
       " 0.00362,\n",
       " 0.004016,\n",
       " 0.003742,\n",
       " 0.002627,\n",
       " 0.00255,\n",
       " 0.002889,\n",
       " 0.002251,\n",
       " 0.002158,\n",
       " 0.00221]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## param count vs time\n",
    "## dataset size? \n",
    "## depth vs time \n",
    "## width vs time\n",
    "## model/loss.backward() vs all the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
